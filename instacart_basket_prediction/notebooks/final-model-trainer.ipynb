{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:57:58.825301Z","iopub.status.busy":"2024-11-11T12:57:58.824888Z","iopub.status.idle":"2024-11-11T12:58:04.001532Z","shell.execute_reply":"2024-11-11T12:58:04.000266Z","shell.execute_reply.started":"2024-11-11T12:57:58.825259Z"},"trusted":true},"outputs":[],"source":["import dagshub\n","import mlflow\n","import os\n","import time\n","import csv\n","import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","import lightgbm as lgb\n","import h2o\n","import glob\n","import gc\n","import matplotlib.pyplot as plt\n","from multiprocessing import Pool\n","from mlflow.models.signature import infer_signature\n","import csv\n","import json\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","from h2o.estimators.gbm import H2OGradientBoostingEstimator\n","from sklearn.metrics import roc_auc_score,precision_score,recall_score,f1_score,accuracy_score,log_loss"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:58:04.003697Z","iopub.status.busy":"2024-11-11T12:58:04.002988Z","iopub.status.idle":"2024-11-11T12:58:04.009266Z","shell.execute_reply":"2024-11-11T12:58:04.007966Z","shell.execute_reply.started":"2024-11-11T12:58:04.003650Z"},"trusted":true},"outputs":[],"source":["# from model_trainer_script import ModelTrainer"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"c45cd753-c853-4f3d-90bd-3fe67b223850","_uuid":"d263f1b5-dd41-4958-ad7f-4ea8f7dedb15","collapsed":false,"execution":{"iopub.execute_input":"2024-11-11T12:58:04.012577Z","iopub.status.busy":"2024-11-11T12:58:04.011285Z","iopub.status.idle":"2024-11-11T12:58:06.251720Z","shell.execute_reply":"2024-11-11T12:58:06.250208Z","shell.execute_reply.started":"2024-11-11T12:58:04.012512Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[38;20m2024-11-11 12:58:05,989 - [INFO]  - [_client.py:1026] - HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"\u001b[0m\n","âœ… Token added successfully\n"]}],"source":["os.environ['MLFLOW_TRACKING_USERNAME'] = 'tnbmarketplace'\n","os.environ['MLFLOW_TRACKING_PASSWORD'] = 'd7c1a451bfc46bf96fbbe5d723becf3955dfa2b3'\n","os.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/tnbmarketplace/instacart_basket_analysis_exp_tracking.mlflow'\n","!dagshub login --token \"d7c1a451bfc46bf96fbbe5d723becf3955dfa2b3\""]},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"486f1b7a-f896-495f-a0fc-e37a428d4f41","_uuid":"3c630320-f370-4792-9391-4a10c15478e9","collapsed":false,"execution":{"iopub.execute_input":"2024-11-11T12:58:06.256867Z","iopub.status.busy":"2024-11-11T12:58:06.256341Z","iopub.status.idle":"2024-11-11T12:58:06.462877Z","shell.execute_reply":"2024-11-11T12:58:06.460652Z","shell.execute_reply.started":"2024-11-11T12:58:06.256812Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as tnbmarketplace\n","</pre>\n"],"text/plain":["Accessing as tnbmarketplace\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"tnbmarketplace/instacart_basket_analysis_exp_tracking\"</span>\n","</pre>\n"],"text/plain":["Initialized MLflow to track repo \u001b[32m\"tnbmarketplace/instacart_basket_analysis_exp_tracking\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository tnbmarketplace/instacart_basket_analysis_exp_tracking initialized!\n","</pre>\n"],"text/plain":["Repository tnbmarketplace/instacart_basket_analysis_exp_tracking initialized!\n"]},"metadata":{},"output_type":"display_data"}],"source":["dagshub.init(repo_name = \"instacart_basket_analysis_exp_tracking\",repo_owner = 'tnbmarketplace')"]},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"eab0b1c4-6879-444e-b78a-5e4ee5d6a399","_uuid":"6ce13dd9-f459-444d-8064-ddb50dbf86db","collapsed":false,"execution":{"iopub.execute_input":"2024-11-11T12:58:06.464784Z","iopub.status.busy":"2024-11-11T12:58:06.464428Z","iopub.status.idle":"2024-11-11T12:58:06.469383Z","shell.execute_reply":"2024-11-11T12:58:06.468207Z","shell.execute_reply.started":"2024-11-11T12:58:06.464746Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# !ngrok authtoken \"2kvaYw5ZiG5bL8iM8YJBVJPk1Ru_3C16mMgmpKEBYb28PPLUe\"\n","# # Start Ngrok tunnel on H2O's port\n","# ngrok_tunnel = ngrok.connect(8787)\n","# print(\"H2O UI accessible at:\", ngrok_tunnel)"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"cc1d14df-9a63-4f45-b625-8c87c1baa5b7","_uuid":"c0b2430b-167c-4cc2-8c02-084422aaf9b6","collapsed":false,"execution":{"iopub.execute_input":"2024-11-11T12:58:06.471559Z","iopub.status.busy":"2024-11-11T12:58:06.471078Z","iopub.status.idle":"2024-11-11T12:58:06.482550Z","shell.execute_reply":"2024-11-11T12:58:06.481484Z","shell.execute_reply.started":"2024-11-11T12:58:06.471509Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def get_time(start):\n","    return time.time() - start"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:58:06.484511Z","iopub.status.busy":"2024-11-11T12:58:06.484026Z","iopub.status.idle":"2024-11-11T12:58:06.496228Z","shell.execute_reply":"2024-11-11T12:58:06.495033Z","shell.execute_reply.started":"2024-11-11T12:58:06.484460Z"},"trusted":true},"outputs":[],"source":["dataset_version = \"1.2\" # dataset . split method\n","model_version = \"1.1.1\" # algorithm . param version . used dataset \n","read_as_xgb_dmatrix =  True\n","train_xgb_gbm= True\n","train_xgb_rf = False"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:58:06.498294Z","iopub.status.busy":"2024-11-11T12:58:06.497787Z","iopub.status.idle":"2024-11-11T12:58:06.520387Z","shell.execute_reply":"2024-11-11T12:58:06.519274Z","shell.execute_reply.started":"2024-11-11T12:58:06.498222Z"},"trusted":true},"outputs":[],"source":["\n","train_features_name = []\n","with open(\"/kaggle/input/final-dataset-generator/train_set_columns.txt\",\"r\") as f:\n","    csv_reader = csv.reader(f)\n","    for row in csv_reader:\n","        train_features_name = row\n","train_features_name.remove('') \n","\n","test_features_name = []\n","with open(\"/kaggle/input/final-dataset-generator/test_set_columns.txt\",\"r\") as f:\n","    csv_reader = csv.reader(f)\n","    for row in csv_reader:\n","        test_features_name = row\n","test_features_name.remove('')  \n","\n","train_label_index = train_features_name.index('reordered')\n","# test_label_index = test_features_name.index('reordered')\n","\n","\n","\n","for i in range(len(test_features_name)):\n","\n","    if test_features_name[i] == 'time_mean_dow_count':\n","       test_features_name[i] ='total_ord_count_p_dow'\n","\n","    if test_features_name[i] == 'time_mean_ohod_count':\n","       test_features_name[i] ='total_ord_count_p_ohod'\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:58:06.522575Z","iopub.status.busy":"2024-11-11T12:58:06.522072Z","iopub.status.idle":"2024-11-11T12:58:07.308703Z","shell.execute_reply":"2024-11-11T12:58:07.307256Z","shell.execute_reply.started":"2024-11-11T12:58:06.522521Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"460f2699ff2d4509a401826e187f9856","version_major":2,"version_minor":0},"text/plain":["Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["xgb_booster = mlflow.xgboost.load_model(\"mlflow-artifacts:/c872cb0ab0ca48d481a71a9bcd2a2102/626567462ef746f8b45a4255ff51c5d1/artifacts/xgb_gbm_model\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:58:07.321297Z","iopub.status.busy":"2024-11-11T12:58:07.316839Z","iopub.status.idle":"2024-11-11T12:58:07.330970Z","shell.execute_reply":"2024-11-11T12:58:07.329363Z","shell.execute_reply.started":"2024-11-11T12:58:07.321228Z"},"trusted":true},"outputs":[],"source":["params = {\n","    'max_depth': 1,\n","    'min_child_weight': 389,\n","    'verbose': -1,\n","    'gamma': 438,\n","    'eta': 0.0907183045377987,\n","    'subsample': 0.10741019033611729,\n","    'colsample_bytree': 0.336917979846298,\n","    'colsample_bylevel': 0.118177830385349,\n","    'lambda': 11,\n","    'alpha': 89,\n","    'booster': 'gbtree',\n","    'tree_method': 'hist',\n","    'objective': 'binary:logistic'\n","}\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:58:07.335096Z","iopub.status.busy":"2024-11-11T12:58:07.333849Z","iopub.status.idle":"2024-11-11T12:59:43.236035Z","shell.execute_reply":"2024-11-11T12:59:43.235031Z","shell.execute_reply.started":"2024-11-11T12:58:07.335020Z"},"trusted":true},"outputs":[],"source":["if read_as_xgb_dmatrix == True:\n","    \n","    train_features_name.remove('reordered')\n","    # test_features_name.remove('reordered')\n","\n","    dtrain_2 = xgb.DMatrix(f\"/kaggle/input/final-dataset-generator/final_prior_train_set.csv/part-00000-37c713df-e906-4480-9322-5a9f58652182-c000.csv?format=csv&label_column={train_label_index}\",nthread=-1,feature_names=train_features_name)\n","    # dtest_2 = xgb.DMatrix(f\"/kaggle/input/final-dataset-generator/featured_test_set.csv/part-00000-16b6977e-6b54-4aff-ad77-cb8ae65dfb2f-c000.csv?format=csv&label_column={test_label_index}\",nthread=-1,feature_names=test_features_name)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:59:43.243811Z","iopub.status.busy":"2024-11-11T12:59:43.241221Z","iopub.status.idle":"2024-11-11T12:59:43.477131Z","shell.execute_reply":"2024-11-11T12:59:43.475566Z","shell.execute_reply.started":"2024-11-11T12:59:43.243753Z"},"trusted":true},"outputs":[],"source":["# %% [code]\n","import time\n","import os\n","import gc\n","import json\n","import mlflow\n","import h2o\n","import xgboost as xgb\n","import lightgbm as lgb\n","from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, log_loss\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","from h2o.estimators import H2OGradientBoostingEstimator\n","\n","class ModelTrainer:\n","    \n","    def __init__(self, experiment_name, train_set, test_set=None, target_column='reordered'):\n","        \"\"\"\n","        Initializes the ModelTrainer with the experiment name, training set, and optional test set.\n","        \n","        Parameters:\n","        experiment_name (str): The name of the MLflow experiment.\n","        train_set (H2OFrame): The training dataset.\n","        test_set (H2OFrame, optional): The test dataset. Defaults to None.\n","        target_column (str): The name of the target column. Defaults to 'reordered'.\n","        \"\"\"\n","        self.train_set = train_set\n","        self.test_set = test_set\n","        self.exp_name = experiment_name\n","        self.target_column = target_column  # Store the target column name\n","        mlflow.set_experiment(self.exp_name)\n","\n","    def __log_details(self, y_true, preds, prev_commit_hash, params, model=None):\n","        \"\"\"\n","        Logs detailed information about the model's performance, environment, and dataset to MLflow.\n","        \n","        Parameters:\n","        y_true (array-like): True values for the target variable.\n","        preds (array-like): Predicted values.\n","        prev_commit_hash (str): The commit hash for version control reference.\n","        params (dict): Model hyperparameters.\n","        model (object, optional): The trained model. Defaults to None.\n","        \n","        Logs:\n","        - Precision, recall, F1 score, AUC, and log loss.\n","        - Commit URL, environment, and dataset details.\n","        \"\"\"\n","        try:\n","            # Log parameters\n","            if params != None:\n","                mlflow.log_params(params)\n","            else:\n","                mlflow.log_param(\"params\", None)\n","\n","            # Log metrics\n","            pred_logits = [1 if pred >= 0.5 else 0 for pred in preds]\n","            mlflow.log_metric(\"precision\", precision_score(y_true, pred_logits))\n","            mlflow.log_metric(\"recall\", recall_score(y_true, pred_logits))\n","            mlflow.log_metric(\"f1\", f1_score(y_true, pred_logits))\n","            mlflow.log_metric(\"AUC\", roc_auc_score(y_true, preds))\n","            mlflow.log_metric(\"logloss\", log_loss(y_true, preds))\n","\n","            # Log script URL with version \n","            commit_url = \"https://github.com/d-sutariya/instacart_next_basket_prediction/tree/\" + prev_commit_hash\n","            mlflow.log_param(\"repository url\", commit_url)\n","\n","            # Log environment \n","            os.system(\"conda env export > conda.yaml\")\n","            mlflow.log_artifact(\"conda.yaml\")\n","\n","            # Log dataset \n","            dataset_path = \"https://www.kaggle.com/datasets/deepsutariya/instacart-exp-data\" \n","            mlflow.log_param(\"dataset url\", dataset_path)\n","\n","            mlflow.end_run()\n","\n","        except Exception as e:\n","            raise RuntimeError(f\"Error logging model details: {str(e)}\")\n","    \n","    def train_h2o_glm(self, prev_commit_hash, dataset_version, model_version, params=None):\n","        \"\"\"\n","        Trains a Generalized Linear Model (GLM) using H2O's binomial family.\n","        \n","        Parameters:\n","        prev_commit_hash (str): The commit hash for version control.\n","        dataset_version (str): The version of the dataset.\n","        model_version (str): The version of the model.\n","        params (dict, optional): Hyperparameters for the model. Defaults to None.\n","        \n","        Returns:\n","        h2o_model: The trained H2O GLM model.\n","        \"\"\"\n","        try:\n","            start = time.time()\n","            \n","            h2o_logistic_model = H2OGeneralizedLinearEstimator(family='binomial') \\\n","                                .train(x=self.train_set.drop(\"reordered\").columns, y='reordered', training_frame=self.train_set)\n","            duration = time.time() - start\n","            \n","            with mlflow.start_run():\n","                mlflow.h2o.log_model(h2o_logistic_model, \"h2o_logistic_model\")\n","                mlflow.log_param(\"family\", \"binomial\")\n","                mlflow.log_param(\"alpha\", h2o_logistic_model.get_params()['alpha'])\n","                mlflow.log_param(\"lambda\", h2o_logistic_model.get_params()['lambda'])\n","                \n","                mlflow.log_param(\"training_time\", duration)\n","                mlflow.set_tag(\"dataset_version\", dataset_version)\n","                mlflow.set_tag(\"model_version\", model_version)\n","                mlflow.set_tag(\"algorithm\", \"h2o_glm\")\n","                \n","                progress = h2o_logistic_model.scoring_history().to_dict()\n","                with open(\"loss_history.json\", \"w\") as f:\n","                    json.dump(progress, f)\n","                mlflow.log_artifact(\"loss_history.json\")\n","                \n","                if self.test_set != None:\n","                    preds = h2o_logistic_model.predict(self.test_set).as_data_frame(use_multi_thread=True)['p1']\n","                    y_true = self.test_set['reordered'].as_data_frame(use_multi_thread=True)\n","                else:\n","                    preds = h2o_logistic_model.predict(self.train_set).as_data_frame(use_multi_thread=True)['p1']\n","                    y_true = self.train_set['reordered'].as_data_frame(use_multi_thread=True)\n","\n","                self.__log_details(y_true, preds, prev_commit_hash, params)\n","                \n","                del y_true, preds\n","                gc.collect()\n","\n","            return h2o_logistic_model\n","        \n","        except Exception as e:\n","            raise RuntimeError(f\"Error training H2O GLM: {str(e)}\")\n","\n","    def train_h2o_gbm(self, prev_commit_hash, dataset_version, model_version, params=None):\n","        \"\"\"\n","        Trains a Gradient Boosting Machine (GBM) model using H2O's GradientBoostingEstimator.\n","        \n","        Parameters:\n","        prev_commit_hash (str): The commit hash for version control.\n","        dataset_version (str): The version of the dataset.\n","        model_version (str): The version of the model.\n","        params (dict, optional): Hyperparameters for the model. Defaults to None.\n","        \n","        Returns:\n","        h2o_model: The trained H2O GBM model.\n","        \"\"\"\n","        try:\n","            if params != None and \"distribution\" not in params.keys():\n","                params['distribution'] = 'bernoulli'\n","\n","            if params is None:\n","                params = {'distribution': 'bernoulli'}\n","            \n","            start = time.time()\n","            \n","            if self.test_set != None:\n","                h2o_gbm = H2OGradientBoostingEstimator(**params) \\\n","                          .train(x=self.train_set.drop(\"reordered\").columns,\n","                                 y='reordered',\n","                                 training_frame=self.train_set,\n","                                 validation_frame=self.test_set)\n","            else:\n","                h2o_gbm = H2OGradientBoostingEstimator(**params) \\\n","                          .train(x=self.train_set.drop(\"reordered\").columns,\n","                                 y='reordered',\n","                                 training_frame=self.train_set)\n","\n","            duration = time.time() - start\n","            \n","            with mlflow.start_run():\n","                mlflow.h2o.log_model(h2o_gbm, \"h2o_gbm_model\")\n","                mlflow.log_params(params if params != None else {})\n","                mlflow.log_param(\"training_time\", duration)\n","                mlflow.set_tag(\"dataset_version\", dataset_version)\n","                mlflow.set_tag(\"model_version\", model_version)\n","                mlflow.set_tag(\"algorithm\", \"h2o_gbm\")\n","                \n","                progress = h2o_gbm.scoring_history().to_dict()\n","                with open(\"loss_history.json\", \"w\") as f:\n","                    json.dump(progress, f)\n","                mlflow.log_artifact(\"loss_history.json\")\n","                \n","                if self.test_set != None:\n","                    preds = h2o_gbm.predict(self.test_set).as_data_frame(use_multi_thread=True)['p1']\n","                    y_true = self.test_set['reordered'].as_data_frame(use_multi_thread=True)\n","                else:\n","                    preds = h2o_gbm.predict(self.train_set).as_data_frame(use_multi_thread=True)['p1']\n","                    y_true = self.train_set['reordered'].as_data_frame(use_multi_thread=True)\n","\n","                self.__log_details(y_true, preds, prev_commit_hash, params, h2o_gbm)\n","\n","            del y_true, preds\n","            gc.collect()\n","            return h2o_gbm\n","        \n","        except Exception as e:\n","            raise RuntimeError(f\"Error training H2O GBM: {str(e)}\")\n","\n","    def train_xgb_gbm(self, prev_commit_hash, dataset_version, model_version, params=None):\n","        \"\"\"\n","        Trains a Gradient Boosting Machine (GBM) using XGBoost.\n","        \n","        Parameters:\n","        prev_commit_hash (str): The commit hash for version control.\n","        dataset_version (str): The version of the dataset.\n","        model_version (str): The version of the model.\n","        params (dict, optional): Hyperparameters for the model. Defaults to None.\n","        \n","        Returns:\n","        xgb_model: The trained XGBoost GBM model.\n","        \"\"\"\n","        try:\n","            if params != None:\n","                if 'booster' not in params.keys():\n","                    params['booster'] = 'gbtree'\n","                if 'tree_method' not in params.keys():\n","                    params['tree_method'] = 'hist'\n","                if 'objective' not in params.keys():\n","                    params['objective'] = 'binary:logistic'\n","                if 'eval_metric' not in params.keys():\n","                    params['eval_metric'] = 'logloss'\n","\n","            start = time.time()\n","\n","            if self.test_set != None:\n","                watchlist = [(self.train_set, 'train'), (self.test_set, 'eval')]\n","                xgb_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, evals=watchlist)\n","            else:\n","                xgb_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30,evals=[(self.train_set,'train')])\n","\n","            duration = time.time() - start\n","\n","            with mlflow.start_run():\n","                \n","                mlflow.xgboost.log_model(xgb_model, \"xgb_gbm_model\")\n","                mlflow.log_params(params if params != None else {})\n","                mlflow.log_param(\"training_time\", duration)\n","                mlflow.set_tag(\"dataset_version\", dataset_version)\n","                mlflow.set_tag(\"model_version\", model_version)\n","                mlflow.set_tag(\"algorithm\", \"xgb_gbm\")\n","\n","                if self.test_set != None:\n","                    preds = xgb_model.predict(self.test_set)\n","                    y_true = self.test_set.get_label()\n","                else:\n","                    preds = xgb_model.predict(self.train_set)\n","                    y_true = self.train_set.get_label()\n","\n","                self.__log_details(y_true, preds, prev_commit_hash, params, xgb_model)\n","\n","            del y_true, preds\n","            gc.collect()\n","            return xgb_model\n","        \n","        except Exception as e:\n","            raise RuntimeError(f\"Error training XGBoost GBM: {str(e)}\")\n","\n","    def train_xgb_rf(self, prev_commit_hash, dataset_version, model_version, params=None):\n","        \"\"\"\n","        Trains a Random Forest using XGBoost's 'random forest' booster.\n","        \n","        Parameters:\n","        prev_commit_hash (str): The commit hash for version control.\n","        dataset_version (str): The version of the dataset.\n","        model_version (str): The version of the model.\n","        params (dict, optional): Hyperparameters for the model. Defaults to None.\n","        \n","        Returns:\n","        xgb_model: The trained XGBoost Random Forest model.\n","        \"\"\"\n","        try:\n","            if params != None:\n","                if 'booster' not in params.keys():\n","                    params['booster'] = 'gbtree'\n","                if 'objective' not in params.keys():\n","                    params['objective'] = 'binary:logistic'\n","                if 'eval_metric' not in params.keys():\n","                    params['eval_metric'] = 'logloss'\n","\n","            start = time.time()\n","           \n","            if self.test_set != None:\n","                watchlist = [(self.train_set, 'train'), (self.test_set, 'eval')]\n","                xgb_rf_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, evals=watchlist)\n","            else:\n","                xgb_rf_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30,evals=[(self.train_set,'train')])\n","\n","            duration = time.time() - start\n","\n","            with mlflow.start_run():\n","                mlflow.xgboost.log_model(xgb_rf_model, \"xgb_rf_model\")\n","                mlflow.log_params(params if params != None else {})\n","                mlflow.log_param(\"training_time\", duration)\n","                mlflow.set_tag(\"dataset_version\", dataset_version)\n","                mlflow.set_tag(\"model_version\", model_version)\n","                mlflow.set_tag(\"algorithm\", \"xgb_rf\")\n","\n","                if self.test_set != None:\n","                    preds = xgb_rf_model.predict(self.test_set)\n","                    y_true = self.test_set.get_label()\n","                else:\n","                    preds = xgb_rf_model.predict(self.train_set)\n","                    y_true = self.test_set.get_label()\n","\n","                self.__log_details(y_true, preds, prev_commit_hash, params, xgb_rf_model)\n","\n","            del y_true, preds\n","            gc.collect()\n","            return xgb_rf_model\n","        \n","        except Exception as e:\n","            raise RuntimeError(f\"Error training XGBoost RF: {str(e)}\")\n","\n","    def train_lgbm(self, prev_commit_hash, dataset_version, model_version, params=None):\n","        \"\"\"\n","        Trains a LightGBM model.\n","        \n","        Parameters:\n","        prev_commit_hash (str): The commit hash for version control.\n","        dataset_version (str): The version of the dataset.\n","        model_version (str): The version of the model.\n","        params (dict, optional): Hyperparameters for the model. Defaults to None.\n","        \n","        Returns:\n","        lgb_model: The trained LightGBM model.\n","        \"\"\"\n","        try:\n","            if params != None:\n","                if 'objective' not in params.keys():\n","                    params['objective'] = 'binary'\n","\n","            start = time.time()\n","\n","            \n","            if self.test_set != None:\n","                lgb_model = lgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, valid_sets=[self.test_set])\n","            else:\n","                lgb_model = lgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, valid_sets=[self.train_set])\n","\n","            duration = time.time() - start\n","\n","            with mlflow.start_run():\n","                \n","                mlflow.lightgbm.log_model(lgb_model, \"lgb_model\")\n","                mlflow.log_params(params if params != None else {})\n","                mlflow.log_param(\"training_time\", duration)\n","                mlflow.set_tag(\"dataset_version\", dataset_version)\n","                mlflow.set_tag(\"model_version\", model_version)\n","                mlflow.set_tag(\"algorithm\", \"lgbm\")\n","\n","            if self.test_set != None:\n","                \n","                preds = lgb_model.predict(self.test_set.get_data())\n","                y_true = self.test_set.get_label()\n","    \n","                self.__log_details(y_true, preds, prev_commit_hash, params, lgb_gbm)\n","            else:\n","                \n","                preds = lgb_model.predict(self.train_set.get_data())\n","                y_true = self.train_set.get_label()\n","    \n","                self.__log_details(y_true, preds, prev_commit_hash, params, lgb_gbm)\n","\n","            del y_true, preds\n","            gc.collect()\n","            return lgb_model\n","        \n","        except Exception as e:\n","            raise RuntimeError(f\"Error training LightGBM: {str(e)}\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-11-11T12:59:43.482551Z","iopub.status.busy":"2024-11-11T12:59:43.482015Z","iopub.status.idle":"2024-11-11T12:59:44.430284Z","shell.execute_reply":"2024-11-11T12:59:44.428627Z","shell.execute_reply.started":"2024-11-11T12:59:43.482489Z"},"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"Error training XGBoost GBM: ('invalid cache item: dict', [<xgboost.core.DMatrix object at 0x7ca942fe4400>, <xgboost.core.DMatrix object at 0x7ca942fe4400>, {'max_depth': 1, 'min_child_weight': 389, 'verbose': -1, 'gamma': 438, 'eta': 0.0907183045377987, 'subsample': 0.10741019033611729, 'colsample_bytree': 0.336917979846298, 'colsample_bylevel': 0.118177830385349, 'lambda': 11, 'alpha': 89, 'booster': 'gbtree', 'tree_method': 'hist', 'objective': 'binary:logistic', 'eval_metric': 'logloss'}])","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 225\u001b[0m, in \u001b[0;36mModelTrainer.train_xgb_gbm\u001b[0;34m(self, prev_commit_hash, dataset_version, model_version, params)\u001b[0m\n\u001b[1;32m    224\u001b[0m     watchlist \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m--> 225\u001b[0m     xgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwatchlist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/training.py:159\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    157\u001b[0m evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(evals) \u001b[38;5;28;01mif\u001b[39;00m evals \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m--> 159\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m start_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:1655\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, cache, model_file)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d, DMatrix):\n\u001b[0;32m-> 1655\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid cache item: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(d)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cache)\n\u001b[1;32m   1657\u001b[0m dmats \u001b[38;5;241m=\u001b[39m c_array(ctypes\u001b[38;5;241m.\u001b[39mc_void_p, [d\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m cache])\n","\u001b[0;31mTypeError\u001b[0m: ('invalid cache item: dict', [<xgboost.core.DMatrix object at 0x7ca942fe4400>, <xgboost.core.DMatrix object at 0x7ca942fe4400>, {'max_depth': 1, 'min_child_weight': 389, 'verbose': -1, 'gamma': 438, 'eta': 0.0907183045377987, 'subsample': 0.10741019033611729, 'colsample_bytree': 0.336917979846298, 'colsample_bylevel': 0.118177830385349, 'lambda': 11, 'alpha': 89, 'booster': 'gbtree', 'tree_method': 'hist', 'objective': 'binary:logistic', 'eval_metric': 'logloss'}])","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m     model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_instacart_training\u001b[39m\u001b[38;5;124m\"\u001b[39m,dtrain_2,params)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# paramss = {\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#     'max_depth':1,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#     'min_child_weight':5,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#     'alpha':100\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     xgb_gbm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_xgb_gbm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc391e8337f10ceb5870cb639159539f5e3497fbf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_xgb_rf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstacart_training\u001b[39m\u001b[38;5;124m\"\u001b[39m,dtrain_2,dtest_2,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreordered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[13], line 254\u001b[0m, in \u001b[0;36mModelTrainer.train_xgb_gbm\u001b[0;34m(self, prev_commit_hash, dataset_version, model_version, params)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xgb_model\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError training XGBoost GBM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: Error training XGBoost GBM: ('invalid cache item: dict', [<xgboost.core.DMatrix object at 0x7ca942fe4400>, <xgboost.core.DMatrix object at 0x7ca942fe4400>, {'max_depth': 1, 'min_child_weight': 389, 'verbose': -1, 'gamma': 438, 'eta': 0.0907183045377987, 'subsample': 0.10741019033611729, 'colsample_bytree': 0.336917979846298, 'colsample_bylevel': 0.118177830385349, 'lambda': 11, 'alpha': 89, 'booster': 'gbtree', 'tree_method': 'hist', 'objective': 'binary:logistic', 'eval_metric': 'logloss'}])"]}],"source":["if train_xgb_gbm == True:\n","    model_trainer = ModelTrainer(\"final_instacart_training\",dtrain_2,params)\n","    # paramss = {\n","    #     'max_depth':1,\n","    #     'min_child_weight':5,\n","    #     'gamma':0.01,\n","    #     'lambda':100,\n","    #     'alpha':100\n","    # }\n","    xgb_gbm = model_trainer.train_xgb_gbm(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version,params)\n","\n","if train_xgb_rf == True:\n","    model_trainer = ModelTrainer(\"instacart_training\",dtrain_2,dtest_2,\"reordered\")\n","    xgb_rf = model_trainer.train_xgb_rf(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version)\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["read_as_lgb_dataset = False\n","train_lgb = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if read_as_lgb_dataset == True:\n","\n","    # train_features_name.remove('reordered')\n","#     test_features_name.remove('reordered')\n","    \n","    dtrain_2 = lgb.Dataset(\"/kaggle/input/final-dataset-generator/final_prior_train_set.csv/part-00000-95d78719-42e1-441c-a4e1-d662676226a3-c000.csv\",params={'label_column':train_label_index},free_raw_data=False).construct()\n","    dtest_2 = pd.read_csv(\"/kaggle/input/final-dataset-generator/featured_test_set.csv/part-00000-1b691469-02d1-4ed1-8b50-ec568a4bf8a9-c000.csv\",header = None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if train_lgb == True:\n","    model_trainer = ModelTrainer(\"final_instacart_training\",[dtrain_2])\n","    lgb_gbm = model_trainer.train_lgb_gbm(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["read_as_h2o_frame = False\n","train_h2o = False"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["if read_as_h2o_frame == True:\n","    h2o.init(max_mem_size = '25G')\n","    dtrain_2 = h2o.import_file(\"/kaggle/input/instacart-dataset-transformer-cv2/final_prior_train_set.csv/part-00000-444aa52f-9c41-4a07-8342-7f82d9fcf6ac-c000.csv\")\n","    dtest_2 = h2o.import_file(\"/kaggle/input/instacart-dataset-transformer-cv2/featured_test_set.csv/part-00000-017a6365-a37c-4ce4-8bf4-96e8daa33570-c000.csv\")\n","    dtrain_2.col_names = train_features_name\n","    dtest_2.col_names = test_features_name\n","    dtrain_2['reordered'] = dtrain_2['reordered'].asfactor()\n","    dtest_2['reordered'] = dtest_2['reordered'].asfactor()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["if train_h2o == True:\n","    \n","    model_trainer = ModelTrainer(\"instacart_training\",[dtrain_2],dtest_2,\"reordered\")\n","    h2o_gbm = model_trainer.train_h2o_glm(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":204944897,"sourceType":"kernelVersion"},{"sourceId":206537344,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
