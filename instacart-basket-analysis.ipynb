{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7487,"sourceType":"datasetVersion","datasetId":4931}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n!pip install pyngrok","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:06:58.253669Z","iopub.execute_input":"2024-10-10T11:06:58.254604Z","iopub.status.idle":"2024-10-10T11:07:57.150817Z","shell.execute_reply.started":"2024-10-10T11:06:58.254550Z","shell.execute_reply":"2024-10-10T11:07:57.149180Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Using cached pyspark-3.5.3.tar.gz (317.3 MB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=46bcf3b6d1916c0ea21fd437388bdfb3689fe3c3232bcd5dc76c0b9aeaddee45\n  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.3\nRequirement already satisfied: pyngrok in /opt/conda/lib/python3.10/site-packages (7.2.0)\nRequirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport pyspark\nimport numpy as np\nfrom pyngrok import ngrok\nfrom pyspark.sql import SparkSession , Window\nfrom pyspark.sql import functions as F\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:07:57.153691Z","iopub.execute_input":"2024-10-10T11:07:57.154254Z","iopub.status.idle":"2024-10-10T11:07:57.366998Z","shell.execute_reply.started":"2024-10-10T11:07:57.154196Z","shell.execute_reply":"2024-10-10T11:07:57.365614Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create a SparkSession with custom memory settings\nspark = SparkSession.builder.appName(\"instamart_analysis\") \\\n    .config(\"spark.driver.memory\",\"25g\") \\\n    .getOrCreate()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:07:57.372605Z","iopub.execute_input":"2024-10-10T11:07:57.373320Z","iopub.status.idle":"2024-10-10T11:08:04.040148Z","shell.execute_reply.started":"2024-10-10T11:07:57.373261Z","shell.execute_reply":"2024-10-10T11:08:04.038544Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/10/10 11:08:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"def show_time(start):\n    return time.time() - start","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:04.043274Z","iopub.execute_input":"2024-10-10T11:08:04.043933Z","iopub.status.idle":"2024-10-10T11:08:04.050530Z","shell.execute_reply.started":"2024-10-10T11:08:04.043882Z","shell.execute_reply":"2024-10-10T11:08:04.049152Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"departments_df = spark.read.options(header=True,inferSchema=True).csv(\"/kaggle/input/instacart-market-basket-analysis/departments.csv\")\nproducts_df = spark.read.options(header=True,inferSchema=True).csv(\"/kaggle/input/instacart-market-basket-analysis/products.csv\")\nprior_product_orders = spark.read.options(header=True,inferSchema=True).csv(\"/kaggle/input/instacart-market-basket-analysis/order_products__prior.csv\").repartition(12)\ntrain_product_orders = spark.read.options(header=True,inferSchema=True).csv(\"/kaggle/input/instacart-market-basket-analysis/order_products__train.csv\").repartition(8)\norders_df = spark.read.options(header=True,inferSchema=True).csv(\"/kaggle/input/instacart-market-basket-analysis/orders.csv\").repartition(8)\naisels_df = spark.read.options(header=True,inferSchema=True).csv(\"/kaggle/input/instacart-market-basket-analysis/aisles.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:04.052591Z","iopub.execute_input":"2024-10-10T11:08:04.053034Z","iopub.status.idle":"2024-10-10T11:08:53.359170Z","shell.execute_reply.started":"2024-10-10T11:08:04.052990Z","shell.execute_reply":"2024-10-10T11:08:53.357768Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Create a tunnel to the Spark UI\nngrok.set_auth_token('2kvaYw5ZiG5bL8iM8YJBVJPk1Ru_3C16mMgmpKEBYb28PPLUe')  # Optional: set your Ngrok auth token if you have one\ntunnel = ngrok.connect(4040)\nprint(\"Ngrok tunnel \\\"{}\\\" -> \\\"http://localhost:4040\\\"\".format(tunnel.public_url))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:53.360785Z","iopub.execute_input":"2024-10-10T11:08:53.361297Z","iopub.status.idle":"2024-10-10T11:08:55.627505Z","shell.execute_reply.started":"2024-10-10T11:08:53.361241Z","shell.execute_reply":"2024-10-10T11:08:55.626190Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Ngrok tunnel \"https://2027-34-147-67-56.ngrok-free.app\" -> \"http://localhost:4040\"                  \n","output_type":"stream"}]},{"cell_type":"code","source":"prior_product_orders.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:55.629209Z","iopub.execute_input":"2024-10-10T11:08:55.629749Z","iopub.status.idle":"2024-10-10T11:08:55.640136Z","shell.execute_reply.started":"2024-10-10T11:08:55.629669Z","shell.execute_reply":"2024-10-10T11:08:55.638831Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"root\n |-- order_id: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- add_to_cart_order: integer (nullable = true)\n |-- reordered: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"orders_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:55.641878Z","iopub.execute_input":"2024-10-10T11:08:55.642280Z","iopub.status.idle":"2024-10-10T11:08:55.649938Z","shell.execute_reply.started":"2024-10-10T11:08:55.642239Z","shell.execute_reply":"2024-10-10T11:08:55.648683Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"root\n |-- order_id: integer (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- eval_set: string (nullable = true)\n |-- order_number: integer (nullable = true)\n |-- order_dow: integer (nullable = true)\n |-- order_hour_of_day: integer (nullable = true)\n |-- days_since_prior_order: double (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"orders_df.cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:55.651368Z","iopub.execute_input":"2024-10-10T11:08:55.651840Z","iopub.status.idle":"2024-10-10T11:08:55.793844Z","shell.execute_reply.started":"2024-10-10T11:08:55.651786Z","shell.execute_reply":"2024-10-10T11:08:55.792228Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"DataFrame[order_id: int, user_id: int, eval_set: string, order_number: int, order_dow: int, order_hour_of_day: int, days_since_prior_order: double]"},"metadata":{}}]},{"cell_type":"code","source":"train_orders_df = orders_df.filter(orders_df[\"eval_set\"] =='train').drop(\"eval_set\")\nprior_orders_df = orders_df.filter(orders_df[\"eval_set\"] == 'prior').drop(\"eval_set\")\ntrain_orders_df.cache()\ntrain_product_orders.cache()\nprior_orders_df.cache()\nprior_product_orders.cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:55.798338Z","iopub.execute_input":"2024-10-10T11:08:55.798829Z","iopub.status.idle":"2024-10-10T11:08:55.971364Z","shell.execute_reply.started":"2024-10-10T11:08:55.798784Z","shell.execute_reply":"2024-10-10T11:08:55.970003Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DataFrame[order_id: int, product_id: int, add_to_cart_order: int, reordered: int]"},"metadata":{}}]},{"cell_type":"code","source":"# how often user has reorderd\ndf_with_num_of_reord = (\n    prior_product_orders.select(\"reordered\",\"order_id\").join(\n        prior_orders_df.select(\"user_id\",\"order_id\"),how=\"left\",on=\"order_id\"\n    ).select(\"user_id\",\"reordered\") \n     .groupBy(\"user_id\").agg(\n        F.count(F.col(\"reordered\")).alias(\"frequency of reorder\")\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:55.973470Z","iopub.execute_input":"2024-10-10T11:08:55.973917Z","iopub.status.idle":"2024-10-10T11:08:56.147072Z","shell.execute_reply.started":"2024-10-10T11:08:55.973875Z","shell.execute_reply":"2024-10-10T11:08:56.145831Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# time since privious order\n\ndf_with_time_since_prev_ord = (\n    prior_orders_df.select(\"user_id\",\"days_since_prior_order\",\"order_hour_of_day\",\"order_number\",\"order_id\") \n                .withColumn(\"privious_order_hour\",\n                            F.lag(\"order_hour_of_day\",1) \n                            .over(Window.partitionBy(\"user_id\").orderBy(\"order_number\"))) \n                .withColumn(\"time_since_Last_order\",\n                            F.col(\"days_since_prior_order\") * 24 + \n                            F.col(\"order_hour_of_day\") - \n                            F.col(\"privious_order_hour\") \n                           ) \n                .select(\"order_id\",\"time_since_last_order\")\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:56.148417Z","iopub.execute_input":"2024-10-10T11:08:56.148919Z","iopub.status.idle":"2024-10-10T11:08:56.329078Z","shell.execute_reply.started":"2024-10-10T11:08:56.148866Z","shell.execute_reply":"2024-10-10T11:08:56.327776Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#time of the day user visits\n\ndf_with_time_of_day_usr_visits = (\n    prior_orders_df.select(\"user_id\" , \"order_hour_of_day\",\"order_id\") \n                .groupBy(\"user_id\",\"order_hour_of_day\") \n                .agg(F.count(\"order_id\").alias(\"frequency\")) \n                .groupBy(\"user_id\") \n                .agg(F.max(\"frequency\").alias(\"maximum_frquency\"))\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:56.330804Z","iopub.execute_input":"2024-10-10T11:08:56.331278Z","iopub.status.idle":"2024-10-10T11:08:56.408853Z","shell.execute_reply.started":"2024-10-10T11:08:56.331229Z","shell.execute_reply":"2024-10-10T11:08:56.407568Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# does the user have ordered asian , gluten free, or organic item \n\ndf_with_does_usr_asian_gluten_orga_items_ord = (\n    prior_product_orders.select(\"order_id\",\"product_id\") \n            .join(products_df.select(\"product_id\",\"product_name\"), on=\"product_id\", how='left') \n            .join(prior_orders_df.select(\"user_id\",\"order_id\"), on=\"order_id\", how='left') \n            .groupBy(\"user_id\", \"order_id\") \n            .agg(F.collect_list(\"product_name\").alias(\"list_of_products\")) \n            .withColumn(\"normalized_list\", F.expr(\"transform(list_of_products, x -> lower(x))\")) \n            .withColumn(\"contains_or_not\", \n                F.expr(\"exists(normalized_list,x -> x like '%organic%')\")\n              | F.expr(\"exists(normalized_list, x -> x like '%asian%')\")\n              | F.expr(\"exists(normalized_list, x-> x like '%gluten free%')\")\n            ) \n            .select(\"order_id\",\"contains_or_not\")\n)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-10T11:17:36.783203Z","iopub.execute_input":"2024-10-10T11:17:36.783712Z","iopub.status.idle":"2024-10-10T11:17:36.908299Z","shell.execute_reply.started":"2024-10-10T11:17:36.783649Z","shell.execute_reply":"2024-10-10T11:17:36.906985Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# feature based on order size \n\ndf_with_fets_of_ord_size = (\n    prior_product_orders.select(\"product_id\",\"order_id\") \n                    .join(prior_orders_df.select(\"user_id\",\"order_id\") , on=\"order_id\", how=\"left\") \n                    .groupBy(\"user_id\",'order_id') \n                    .agg(\n                            F.count(F.col(\"product_id\")).alias(\"count_of_product\")\n                        ) \n                    .groupBy(\"user_id\") \n                    .agg(\n                            F.max(F.col(\"count_of_product\")).alias(\"max_count_of_products\"),\n                            F.min(F.col(\"count_of_product\")).alias(\"min_count_of_products\"),\n                            F.mean(F.col(\"count_of_product\")).alias(\"mean_count_of_products\")\n                        ) \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:56.763014Z","iopub.execute_input":"2024-10-10T11:08:56.764439Z","iopub.status.idle":"2024-10-10T11:08:56.879308Z","shell.execute_reply.started":"2024-10-10T11:08:56.764381Z","shell.execute_reply":"2024-10-10T11:08:56.878149Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# How many of the userâ€™s orders contained no previously purchased items\n\ndf_with_freq_ord_that_hasnt_prev_purch_items = (\n    prior_product_orders.select(\"order_id\",\"reordered\") \n                    .join(prior_orders_df.select(\"order_id\",\"user_id\") , on = 'order_id' , how = 'left') \n                    .groupBy(\"user_Id\",\"order_id\") \n                    .agg(\n                            F.collect_list(F.col(\"reordered\")).alias(\"reordered_array\")\n                        ) \n                    .withColumn(\"doesnt_contains_reordered\" ,\n                            F.when(F.array_contains(\"reordered_array\",1),0).otherwise(1)\n                        ) \n                    .select(\"order_id\",\"doesnt_contains_reordered\")\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:18:45.077558Z","iopub.execute_input":"2024-10-10T11:18:45.078135Z","iopub.status.idle":"2024-10-10T11:18:45.168031Z","shell.execute_reply.started":"2024-10-10T11:18:45.078088Z","shell.execute_reply":"2024-10-10T11:18:45.166663Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# how often the item has been purchaced \n\ndf_with_freq_purch= (\n    prior_product_orders.select(\"product_id\",\"order_id\") \n                     .groupBy(\"product_id\") \n                     .agg(\n                             F.count(F.col(\"order_id\")).alias(\"product_count\")\n                        ) \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.002779Z","iopub.execute_input":"2024-10-10T11:08:57.004238Z","iopub.status.idle":"2024-10-10T11:08:57.045589Z","shell.execute_reply.started":"2024-10-10T11:08:57.004175Z","shell.execute_reply":"2024-10-10T11:08:57.044401Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# position of product \n\ndf_with_avg_position_of_prod = (\n    prior_product_orders.select(\"product_id\",\"add_to_cart_order\") \n                    .groupBy(\"product_id\") \n                    .agg(\n                            F.mean(F.col(\"add_to_cart_order\")).alias(\"product_mean_of_position\")\n                        ) \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.046898Z","iopub.execute_input":"2024-10-10T11:08:57.047683Z","iopub.status.idle":"2024-10-10T11:08:57.086970Z","shell.execute_reply.started":"2024-10-10T11:08:57.047623Z","shell.execute_reply":"2024-10-10T11:08:57.085493Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# How many users buy it as \"one shot\" item\n\ndf_with_freq_one_shot_ord_prods = (\n    prior_product_orders.select(\"order_id\",\"product_id\") \n                    .groupBy(\"order_id\") \n                    .agg(F.collect_list(\"product_id\").alias(\"list_of_products\")) \n                    .withColumn(\"is_one_shot_order\",\n                                   F.when(F.size(F.col(\"list_of_products\")) == 1,1).otherwise(0)\n                               ) \n                    .withColumn(\"product_id\",F.explode(F.col(\"list_of_products\"))) \n                    .join(prior_orders_df.select(\"user_id\",\"order_id\"),on=\"order_id\",how='left') \n                    .groupBy(\"product_id\",\"user_id\") \n                    .agg(F.collect_list(F.col(\"is_one_shot_order\")).alias(\"is_one_shot_order_list\")) \n                    .withColumn(\"has_user_purchased_one_shot\",F.when(F.array_contains(\"is_one_shot_order_list\",1),1).otherwise(0)) \n                    .groupBy(\"product_id\") \n                    .agg(\n                            F.sum(F.col(\"has_user_purchased_one_shot\")).alias(\"number_of_user_purchased_item\")\n                        ) \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.088420Z","iopub.execute_input":"2024-10-10T11:08:57.089780Z","iopub.status.idle":"2024-10-10T11:08:57.305825Z","shell.execute_reply.started":"2024-10-10T11:08:57.089694Z","shell.execute_reply":"2024-10-10T11:08:57.304503Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Stats on the number of items that co-occur with this item\n\n# 1. number of time that a item has co occured.\n\n# Perform a self-join on prior_product_orders\ndf_with_freq_co_ocrd = (\n    prior_product_orders\n    .select(\"product_id\", \"order_id\")\n    .alias(\"df1\")\n    .join(\n        prior_product_orders.select(\"product_id\", \"order_id\")\n        .withColumnRenamed(\"product_id\", \"product_id_1\")\n        .alias(\"df2\"),\n        (F.col(\"df1.order_id\") == F.col(\"df2.order_id\")) & (F.col(\"df1.product_id\") != F.col(\"df2.product_id_1\")),\n        \"left\"\n    )\n    .groupBy(\"df1.product_id\")\n    .agg(F.count(F.col(\"df2.product_id_1\")).alias(\"number_of_product_co_occurred\"))\n)\n\n# 2 average number of items that is co ocuured with this item in single order\n\ndf_with_avg_num_item_co_ocrd_in_ord = (\n                prior_product_orders.select(\"product_id\",\"order_id\").alias(\"ppo1\") \n                .join(\n                    prior_product_orders.select(\"product_id\",\"order_id\")\n                    .alias(\"ppo2\"),\n                    (F.col(\"ppo1.order_id\") == F.col(\"ppo2.order_id\")) & \n                    (F.col(\"ppo1.product_id\") != F.col(\"ppo2.product_id\")),\n                    how='left'\n                ) \n                .groupBy(\"ppo1.product_id\",\"ppo1.order_id\")\n                .agg(F.count(F.col(\"ppo2.product_id\")).alias(\"count_of_co_ocuured_product_per_order\"))\n                .groupBy(\"ppo1.product_id\")\n                .agg(\n                    F.mean(F.col(\"count_of_co_ocuured_product_per_order\")).alias(\"mean_of_co_ocuured_product_per_order\"),\n                    F.min(F.col(\"count_of_co_ocuured_product_per_order\")).alias(\"min_of_co_ocuured_product_per_order\"),\n                    F.max(F.col(\"count_of_co_ocuured_product_per_order\")).alias(\"max_of_co_ocuured_product_per_order\"),\n\n                )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.307340Z","iopub.execute_input":"2024-10-10T11:08:57.307952Z","iopub.status.idle":"2024-10-10T11:08:57.561879Z","shell.execute_reply.started":"2024-10-10T11:08:57.307895Z","shell.execute_reply":"2024-10-10T11:08:57.559164Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Stats on the order streak\n\n# 1. let's add the flag whether streak is continued or not\n\ndf_with_flag= (\n\n    prior_product_orders.select(\"product_id\",\"order_id\")\n                        .join(\n                                prior_orders_df.select(\"user_id\",\"order_number\",\"order_id\"),\n                                how ='left',\n                                on = 'order_id' \n                            )\n                        .withColumn(\"next_order_number\",\n                            F.lead(F.col(\"order_number\"),1).over(Window.partitionBy(\"user_id\",\"product_id\").orderBy(\"order_number\"))\n                        )\n                        .withColumn(\"is_streak_continued_flag\",\n                               F.when(F.col(\"next_order_number\") - F.col(\"order_number\") == 1,1)\n                                    .otherwise(0)\n                            )\n)\n# 2. let's assign an unique id to each streak of a perticular user and product.\n\nw1 = Window.partitionBy(\"user_id\",\"product_id\").orderBy(\"order_number\")\nw2 = Window.partitionBy(\"user_id\",\"product_id\",\"is_streak_continued_flag\").orderBy(\"order_number\")\n\n# by using the above window we can create unique id for streak named grp then can find streak leangth.\ndf_with_streak_length = (\n    df_with_flag.withColumn(\"grp\",F.row_number().over(w1) - F.row_number().over(w2))\n                .groupBy(\"user_id\",\"product_id\",\"grp\")\n                .agg(\n                    F.count(\"order_number\").alias(\"length_of_streaks\")\n                )\n)\n\n# finally , summarize it over each prodcut rather than per user per product.\ndf_with_stats_of_streaks = (\n    df_with_streak_length.select(\"product_id\",\"length_of_streaks\",\"grp\")\n                         .groupBy(\"product_id\")\n                         .agg(\n                             F.count('grp').alias(\"Total_streak_of_this_product\"),\n                             F.mean(\"length_of_streaks\").alias(\"mean_of_streaks_of_this_product\"),\n                             F.min(\"length_of_streaks\").alias(\"max_of_streaks_of_this_product\"),\n                             F.max(\"length_of_streaks\").alias(\"min_of_streaks_of_this_product\")\n                         \n                         )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.565203Z","iopub.execute_input":"2024-10-10T11:08:57.565711Z","iopub.status.idle":"2024-10-10T11:08:57.843888Z","shell.execute_reply.started":"2024-10-10T11:08:57.565658Z","shell.execute_reply":"2024-10-10T11:08:57.842407Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Probability of being reordered within N orders\n\n# we have already counted the lenght of the streaks so if it is >= 5 then it will be added in probability.\n\ndf_with_prob_greater_5 = (\n    df_with_streak_length.withColumn(\"is_streak_length_greater_than_5\",\n                                        F.when(F.col(\"length_of_streaks\") >= 5,1).otherwise(0) \n                                    )\n                         .groupBy(\"product_id\")\n                         .agg(\n                             F.count(\"length_of_streaks\").alias(\"total_streaks\"),\n                             F.sum(\"is_streak_length_greater_than_5\").alias(\"total_streaks_greater_than_5\")\n                         )\n                         .withColumn(\"prob_of_reordered_5\",\n                             ( F.col(\"total_streaks_greater_than_5\") / F.col(\"total_streaks\"))\n                         )\n                         .select(\"product_id\",\"prob_of_reordered_5\")\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.845817Z","iopub.execute_input":"2024-10-10T11:08:57.846363Z","iopub.status.idle":"2024-10-10T11:08:57.937121Z","shell.execute_reply.started":"2024-10-10T11:08:57.846288Z","shell.execute_reply":"2024-10-10T11:08:57.935776Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# we have already counted the lenght of the streaks so if it is >= 2 then it will be added in probability.\n\ndf_with_prob_greater_2 = (\n    df_with_streak_length.withColumn(\"is_streak_length_greater_than_2\",\n                                        F.when(F.col(\"length_of_streaks\") >= 2,1).otherwise(0) \n                                    )\n                         .groupBy(\"product_id\")\n                         .agg(\n                             F.count(\"length_of_streaks\").alias(\"total_streaks\"),\n                             F.sum(\"is_streak_length_greater_than_2\").alias(\"total_streaks_greater_than_2\")\n                         )\n                         .withColumn(\"prob_of_reordered_2\",\n                             ( F.col(\"total_streaks_greater_than_2\") / F.col(\"total_streaks\"))\n                         )\n                         .select(\"product_id\",\"prob_of_reordered_2\")\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:57.938525Z","iopub.execute_input":"2024-10-10T11:08:57.938929Z","iopub.status.idle":"2024-10-10T11:08:58.015688Z","shell.execute_reply.started":"2024-10-10T11:08:57.938890Z","shell.execute_reply":"2024-10-10T11:08:58.014243Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# we have already counted the lenght of the streaks so if it is >= 3 then it will be added in probability.\n\ndf_with_prob_greater_3 = (\n    df_with_streak_length.withColumn(\"is_streak_length_greater_than_3\",\n                                        F.when(F.col(\"length_of_streaks\") >= 3,1).otherwise(0) \n                                    )\n                         .groupBy(\"product_id\")\n                         .agg(\n                             F.count(\"length_of_streaks\").alias(\"total_streaks\"),\n                             F.sum(\"is_streak_length_greater_than_3\").alias(\"total_streaks_greater_than_3\")\n                         )\n                         .withColumn(\"prob_of_reordered_3\",\n                             ( F.col(\"total_streaks_greater_than_3\") / F.col(\"total_streaks\"))\n                         )\n                         .select(\"product_id\",\"prob_of_reordered_3\")\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:08:58.017426Z","iopub.execute_input":"2024-10-10T11:08:58.018863Z","iopub.status.idle":"2024-10-10T11:08:58.098604Z","shell.execute_reply.started":"2024-10-10T11:08:58.018800Z","shell.execute_reply":"2024-10-10T11:08:58.097480Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Distribution of the day of week it is ordered\npivoted_prior_orders_df = (\n    prior_orders_df.select(\"order_id\",\"order_dow\")\n                    .groupBy(\"order_id\")\n                    .pivot(\"order_dow\")\n                    .agg(F.lit(1)).na.fill(0)\n)\n            \ndf_with_count_of_dow = (\n    prior_product_orders.select(\"order_id\",\"product_id\")\n                            .join(\n                                pivoted_prior_orders_df , on = \"order_id\",how='left'\n                            )\n                            .groupBy(\"product_id\")\n                            .agg(\n                                F.sum(\"0\").alias(\"count_of_dow_0\"),\n                                F.sum(\"1\").alias(\"count_of_dow_1\"),\n                                F.sum(\"2\").alias(\"count_of_dow_2\"),\n                                F.sum(\"3\").alias(\"count_of_dow_3\"),\n                                F.sum(\"4\").alias(\"count_of_dow_4\"),\n                                F.sum(\"5\").alias(\"count_of_dow_5\"),\n                                F.sum(\"6\").alias(\"count_of_dow_6\")\n                            )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:39:26.307947Z","iopub.execute_input":"2024-10-10T11:39:26.308854Z","iopub.status.idle":"2024-10-10T11:39:27.403591Z","shell.execute_reply.started":"2024-10-10T11:39:26.308806Z","shell.execute_reply":"2024-10-10T11:39:27.402541Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"#  Probability it is reordered after the first order\ntotal_orders = prior_orders_df.select(\"order_id\").distinct().count()\n\ndf_with_prob_reord = (\n    prior_orders_df.select(\"order_id\",\"user_id\")\n                    .join(prior_product_orders.select(\"product_id\",\"order_id\"),on=\"order_id\",how='left')\n                    .groupBy(\"product_id\",\"user_id\")\n                    .agg(\n                        F.count(\"order_id\").alias(\"order_count\")\n                    )\n                    .groupBy(\"product_id\")\n                    .agg(\n                        ( \n                            (F.sum(\"order_count\") / total_orders).alias(\"prob_of_being_reordered\") \n                        )\n                    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:09:17.824421Z","iopub.execute_input":"2024-10-10T11:09:17.824913Z","iopub.status.idle":"2024-10-10T11:09:22.631223Z","shell.execute_reply.started":"2024-10-10T11:09:17.824859Z","shell.execute_reply":"2024-10-10T11:09:22.629938Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Number of orders in which the user purchases the item\n\ndf_with_num_of_order_p_product = (\n    \n    prior_product_orders.select(\"order_id\",\"product_id\")\n                        .join(\n                            prior_orders_df.select(\"order_id\",\"user_id\")\n                            , how = 'left' , on = 'order_id'\n                        )\n                        .groupBy(\"user_id\",\"product_id\")\n                        .agg(\n                            F.count(\"order_id\").alias(\"num_of_ord_purch_p_prod\")\n                        )\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:09:22.632534Z","iopub.execute_input":"2024-10-10T11:09:22.633004Z","iopub.status.idle":"2024-10-10T11:09:22.705783Z","shell.execute_reply.started":"2024-10-10T11:09:22.632951Z","shell.execute_reply":"2024-10-10T11:09:22.704263Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# # Days since the user last purchased the item\n\n# w1 = Window.partitionBy(\"user_id\",\"product_id\").orderBy(\"order_number\")\n# df_with_next_order_p_prod = (\n#     prior_product_orders.select(\"product_id\",\"order_id\")\n#                         .join(\n#                             prior_orders_df.select(\"user_id\",\"order_id\",\"order_number\",\"days_since_prior_order\")\n#                                             .groupBy(\"user_id\")\n#                                             .agg(\n#                                                 F.collect_list(\"days_since_prior_order\").alias(\"list_of_days_since_prior_ord\"),\n#                                                 F.collect_list(\"order_number\").alias(\"list_of_order_number\")\n#                                             )\n#                             ,\n#                             how='left',on='order_id'\n#                         )\n#                         .withColumn(\"pre_order_number\",\n#                             F.lag(F.col(\"order_number\")).over(w1)\n                                \n#                         ).na.fill({'pre_order_number':0})\n#                         .sort(\"user_id\",\"product_id\",\"order_number\")\n#                         .show()\n                        \n# )\n\n# w2 = Window.partitionBy(\"user_id\").orderBy(\"order_number\") \n#                         .rowsBetween(\n#                             F.when(F.col(\"pre_order_number\") == 0,Window.unboundedPreceding).otherwise(F.col(\"pre_order_number\")) ,\n#                             F.col(\"order_number\")\n#                         )\n\n# df_with_days_since_last_ord_p_prod = (\n#     df_with_next_order_p_prod.join(\n#                                 prior_orders_df.select(\"user_id\",\"days_since_prior_orders\"),\n#                                 how='left',on='user_id'\n#                             ).sort(\"user_id\",\"order\")\n#                             .groupBy(\"user_id\")\n#                             .withColum(\"sum_day_since_last_order\",\n#                                 F.sum(\"days_since_prior_order\").over(w2)\n#                             )\n# )","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:09:22.716184Z","iopub.execute_input":"2024-10-10T11:09:22.716751Z","iopub.status.idle":"2024-10-10T11:09:22.724370Z","shell.execute_reply.started":"2024-10-10T11:09:22.716678Z","shell.execute_reply":"2024-10-10T11:09:22.723181Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Position in the cart\ndf_with_position_cart_p_usr_p_prod = (\n    prior_product_orders.select(\"product_id\",\"add_to_cart_order\",\"order_id\") \n                    .join(\n                        prior_orders_df.select(\"user_id\",\"order_id\"),\n                        how = 'left' , on = 'order_id'\n                    )\n                    .groupBy(\"user_id\",\"product_id\") \n                    .agg(\n                            F.mean(F.col(\"add_to_cart_order\")).alias(\"prod_mean_of_position_p_user\")\n                        )\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:58:35.088558Z","iopub.execute_input":"2024-10-10T11:58:35.089106Z","iopub.status.idle":"2024-10-10T11:58:35.140027Z","shell.execute_reply.started":"2024-10-10T11:58:35.089063Z","shell.execute_reply":"2024-10-10T11:58:35.138813Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Co-occurrence statistics\n\ndf_with_co_ocrd_stats_p_user_p_prod = (\n    prior_product_orders\n    .select(\"product_id\", \"order_id\")\n    .alias(\"df1\")\n    .join(prior_orders_df.select(\"user_id\",\"order_id\"),\n         on = 'order_id',how='left'\n         )\n    .join(\n        prior_product_orders.select(\"product_id\", \"order_id\")\n        .withColumnRenamed(\"product_id\", \"product_id_1\")\n        .alias(\"df2\"),\n        (F.col(\"df1.order_id\") == F.col(\"df2.order_id\")) & (F.col(\"df1.product_id\") != F.col(\"df2.product_id_1\")),\n        \"left\"\n    )\n    .groupBy(\"user_id\",\"df1.product_id\")\n    .agg(\n        F.count(F.col(\"df2.product_id_1\")).alias(\"num_of_prod_co_ocrd_p_usr_p_prod\"),\n    )\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T12:12:04.359305Z","iopub.execute_input":"2024-10-10T12:12:04.359857Z","iopub.status.idle":"2024-10-10T12:12:04.432649Z","shell.execute_reply.started":"2024-10-10T12:12:04.359811Z","shell.execute_reply":"2024-10-10T12:12:04.431499Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"#Counts by day of wee\n\ndf_with_count_of_dow = (\n        prior_orders_df.select(\"order_id\",\"order_dow\")\n                        .groupBy(\"order_id\")\n                        .pivot(\"order_dow\")\n                        .agg(F.lit(1))\n                        .na.fill(0)\n                        .agg(*\n                              [\n                                F.sum(f\"{i}\").alias(f\"count_dow_{i}\") for i in range(7)\n                                ]\n                            )\n)\n\n#Counts by hour\n\ndf_with_count_of_ohod = (\n        prior_orders_df.select(\"order_id\",\"order_hour_of_day\")\n                        .groupBy(\"order_id\")\n                        .pivot(\"order_hour_of_day\")\n                        .agg(F.lit(1))\n                        .na.fill(0)\n                        .agg(*\n                              [\n                                F.sum(f\"{i}\").alias(f\"count_ohod_{i}\") for i in range(24)\n                                ]\n                            )\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T12:24:53.149031Z","iopub.execute_input":"2024-10-10T12:24:53.149529Z","iopub.status.idle":"2024-10-10T12:24:54.562974Z","shell.execute_reply.started":"2024-10-10T12:24:53.149481Z","shell.execute_reply":"2024-10-10T12:24:54.561526Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"result_df = (\n    prior_orders_df.join(\n        df_with_num_of_reord , on = \"user_id\" ,how = 'left'\n    )\n    .join(\n        df_with_time_since_prev_ord , on = \"order_id\" , how = \"left\"\n    )\n    .join(\n        df_with_does_usr_asian_gluten_orga_items_ord , on = \"order_id\" , how = 'left'\n    )\n    .join(\n        df_with_fets_of_ord_size , on = 'user_id' , how = 'left'\n    )\n    .join(\n        df_with_freq_ord_that_hasnt_prev_purch_items , on = \"order_id\",how='left'\n    )\n)\nresult_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:22:18.657322Z","iopub.execute_input":"2024-10-10T11:22:18.657834Z","iopub.status.idle":"2024-10-10T11:22:18.745622Z","shell.execute_reply.started":"2024-10-10T11:22:18.657788Z","shell.execute_reply":"2024-10-10T11:22:18.743989Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"root\n |-- order_id: integer (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- order_number: integer (nullable = true)\n |-- order_dow: integer (nullable = true)\n |-- order_hour_of_day: integer (nullable = true)\n |-- days_since_prior_order: double (nullable = true)\n |-- frequency of reorder: long (nullable = true)\n |-- time_since_last_order: double (nullable = true)\n |-- contains_or_not: boolean (nullable = true)\n |-- max_count_of_products: long (nullable = true)\n |-- min_count_of_products: long (nullable = true)\n |-- mean_count_of_products: double (nullable = true)\n |-- doesnt_contains_reordered: integer (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"result_df.cache()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:26:56.258643Z","iopub.execute_input":"2024-10-10T11:26:56.259205Z","iopub.status.idle":"2024-10-10T11:26:56.487657Z","shell.execute_reply.started":"2024-10-10T11:26:56.259165Z","shell.execute_reply":"2024-10-10T11:26:56.486380Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"DataFrame[order_id: int, user_id: int, order_number: int, order_dow: int, order_hour_of_day: int, days_since_prior_order: double, frequency of reorder: bigint, time_since_last_order: double, contains_or_not: boolean, max_count_of_products: bigint, min_count_of_products: bigint, mean_count_of_products: double, doesnt_contains_reordered: int]"},"metadata":{}}]},{"cell_type":"code","source":"result_df.count()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:27:02.066938Z","iopub.execute_input":"2024-10-10T11:27:02.067593Z","iopub.status.idle":"2024-10-10T11:29:23.866297Z","shell.execute_reply.started":"2024-10-10T11:27:02.067542Z","shell.execute_reply":"2024-10-10T11:29:23.864531Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"3214874"},"metadata":{}}]},{"cell_type":"code","source":"result_product_df = (\n    df_with_avg_position_of_prod\n    .join(\n        df_with_freq_one_shot_ord_prods , on = 'product_id' , how = 'left'\n    )\n    .join(\n        df_with_freq_co_ocrd , on = \"product_id\" , how = 'left'\n    )\n    .join(\n        df_with_avg_num_item_co_ocrd_in_ord , df_with_avg_num_item_co_ocrd_in_ord[\"ppo1.product_id\"] == prior_product_orders[\"product_id\"] , how =\"left\"\n    )\n    .join(\n        df_with_stats_of_streaks , on = 'product_id' , how = 'left'\n    )\n    .join(\n        df_with_prob_greater_5 , on = 'product_id' , how = \"left\"\n    )\n    .join(\n        df_with_prob_greater_3 , on = 'product_id' , how = \"left\"\n    )\n    .join(\n        df_with_prob_greater_2 , on = 'product_id' , how = \"left\"\n    )\n    .join(\n        df_with_count_of_dow , on = 'product_id', how = 'left'\n    )\n    .join(\n        df_with_prob_reord , on = 'product_id' , how = 'left'\n    )\n    .join(\n        df_with_num_of_order_p_product , on = 'product_id' , how = 'left'\n    )\n)\n\nresult_product_df.cache()\nresult_product_df.count()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T11:44:48.844274Z","iopub.execute_input":"2024-10-10T11:44:48.844754Z","iopub.status.idle":"2024-10-10T11:55:38.680585Z","shell.execute_reply.started":"2024-10-10T11:44:48.844689Z","shell.execute_reply":"2024-10-10T11:55:38.679205Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"13307953"},"metadata":{}}]},{"cell_type":"code","source":"result_usr_prod_df = (\n    prior_product_orders\n    .withColumnRenamed(\"product_id\",\"product_id_p\")\n    .alias(\"ppo\")\n    .join(\n        prior_orders_df.select(\"user_id\",\"order_id\").withColumnRenamed(\"user_id\",\"user_id_p\").alias(\"pod\") , on = 'order_id' , how = 'left'\n    )\n    .join(\n        df_with_num_of_order_p_product ,\n            (F.col(\"pod.user_id_p\") == df_with_num_of_order_p_product['user_id']) &\n            (F.col(\"ppo.product_id_p\") == df_with_num_of_order_p_product['product_id']) \n        , how = 'left'\n    ).drop(\"user_id\",\"product_id\")\n    .join(\n        df_with_position_cart_p_usr_p_prod ,\n        (df_with_position_cart_p_usr_p_prod[\"user_id\"] == F.col(\"pod.user_id_p\")) &\n        (df_with_position_cart_p_usr_p_prod[\"product_id\"] == F.col(\"ppo.product_id_p\"))\n        ,how = 'left'\n    ).drop(\"user_id\",\"product_id\")\n    .join(\n        df_with_co_ocrd_stats_p_user_p_prod ,\n        (df_with_co_ocrd_stats_p_user_p_prod[\"user_id\"] == F.col(\"pod.user_id_p\") ) &\n        (df_with_co_ocrd_stats_p_user_p_prod[\"product_id\"] == F.col(\"ppo.product_id_p\"))\n        , how = 'left'\n    ).drop(\"user_id\",\"product_id\")\n)\n\nresult_usr_prod_df.cache()\nresult_usr_prod_df.count()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T12:12:54.086923Z","iopub.execute_input":"2024-10-10T12:12:54.087371Z","iopub.status.idle":"2024-10-10T12:17:16.684703Z","shell.execute_reply.started":"2024-10-10T12:12:54.087330Z","shell.execute_reply":"2024-10-10T12:17:16.683527Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"32434489"},"metadata":{}}]},{"cell_type":"code","source":"result_df_with_time_df = (\n    result_df.crossJoin(\n        F.broadcast(df_with_count_of_dow)\n    )\n    .crossJoin(\n        F.broadcast(df_with_count_of_ohod)\n    )\n)\nresult_df_with_time_df.cache()\nresult_df_with_time_df.count()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T12:25:24.034500Z","iopub.execute_input":"2024-10-10T12:25:24.035002Z","iopub.status.idle":"2024-10-10T12:26:02.917814Z","shell.execute_reply.started":"2024-10-10T12:25:24.034958Z","shell.execute_reply":"2024-10-10T12:26:02.916438Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"3214874"},"metadata":{}}]},{"cell_type":"code","source":"result_product_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T12:29:45.104000Z","iopub.execute_input":"2024-10-10T12:29:45.105219Z","iopub.status.idle":"2024-10-10T12:29:45.112775Z","shell.execute_reply.started":"2024-10-10T12:29:45.105167Z","shell.execute_reply":"2024-10-10T12:29:45.111567Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"root\n |-- product_id: integer (nullable = true)\n |-- product_mean_of_position: double (nullable = true)\n |-- number_of_user_purchased_item: long (nullable = true)\n |-- number_of_product_co_occurred: long (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- mean_of_co_ocuured_product_per_order: double (nullable = true)\n |-- min_of_co_ocuured_product_per_order: long (nullable = true)\n |-- max_of_co_ocuured_product_per_order: long (nullable = true)\n |-- Total_streak_of_this_product: long (nullable = true)\n |-- mean_of_streaks_of_this_product: double (nullable = true)\n |-- max_of_streaks_of_this_product: long (nullable = true)\n |-- min_of_streaks_of_this_product: long (nullable = true)\n |-- prob_of_reordered_5: double (nullable = true)\n |-- prob_of_reordered_3: double (nullable = true)\n |-- prob_of_reordered_2: double (nullable = true)\n |-- count_of_dow_0: long (nullable = true)\n |-- count_of_dow_1: long (nullable = true)\n |-- count_of_dow_2: long (nullable = true)\n |-- count_of_dow_3: long (nullable = true)\n |-- count_of_dow_4: long (nullable = true)\n |-- count_of_dow_5: long (nullable = true)\n |-- count_of_dow_6: long (nullable = true)\n |-- prob_of_being_reordered: double (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- num_of_ord_purch_p_prod: long (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"result_usr_prod_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T12:30:17.618912Z","iopub.execute_input":"2024-10-10T12:30:17.619774Z","iopub.status.idle":"2024-10-10T12:30:17.629149Z","shell.execute_reply.started":"2024-10-10T12:30:17.619693Z","shell.execute_reply":"2024-10-10T12:30:17.627784Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"root\n |-- order_id: integer (nullable = true)\n |-- product_id_p: integer (nullable = true)\n |-- add_to_cart_order: integer (nullable = true)\n |-- reordered: integer (nullable = true)\n |-- user_id_p: integer (nullable = true)\n |-- num_of_ord_purch_p_prod: long (nullable = true)\n |-- prod_mean_of_position_p_user: double (nullable = true)\n |-- num_of_prod_co_ocrd_p_usr_p_prod: long (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"code","source":"final_prior_ord_train_df = (\n    result_usr_prod_df.join(\n        result_df_with_time_df.drop(\"user_id\"),\n        on = \"order_id\",how='left'\n    )\n    .join(\n        result_product_df.drop(\"user_id\") ,\n        (F.col(\"product_id_p\") == result_product_df['ppo1.product_id'])\n        , how = 'left'\n    ).drop(\"product_id\")\n)\nfinal_prior_ord_train_df.cache()\nfinal_prior_ord_train_df.count()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:07:23.733449Z","iopub.execute_input":"2024-10-10T13:07:23.733960Z","iopub.status.idle":"2024-10-10T13:08:42.844092Z","shell.execute_reply.started":"2024-10-10T13:07:23.733914Z","shell.execute_reply":"2024-10-10T13:08:42.842116Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stderr","text":"24/10/10 13:07:23 WARN CacheManager: Asked to cache already cached data.\n24/10/10 13:08:42 WARN BlockManager: Putting block rdd_932_0 failed due to exception org.apache.spark.TaskKilledException.\n24/10/10 13:08:42 WARN BlockManager: Block rdd_932_0 could not be removed as it was not found on disk or in memory\n24/10/10 13:08:42 WARN TaskSetManager: Lost task 0.0 in stage 909.0 (TID 3237) (8da198fe9dfc executor driver): TaskKilled (Stage cancelled: Job 176 cancelled )\n24/10/10 13:08:42 WARN BlockManager: Putting block rdd_932_3 failed due to exception org.apache.spark.TaskKilledException.\n24/10/10 13:08:42 WARN BlockManager: Block rdd_932_3 could not be removed as it was not found on disk or in memory\n24/10/10 13:08:42 WARN TaskSetManager: Lost task 3.0 in stage 909.0 (TID 3240) (8da198fe9dfc executor driver): TaskKilled (Stage cancelled: Job 176 cancelled )\n24/10/10 13:08:42 WARN BlockManager: Putting block rdd_932_2 failed due to exception org.apache.spark.TaskKilledException.\n24/10/10 13:08:42 WARN BlockManager: Block rdd_932_2 could not be removed as it was not found on disk or in memory\n24/10/10 13:08:42 WARN TaskSetManager: Lost task 2.0 in stage 909.0 (TID 3239) (8da198fe9dfc executor driver): TaskKilled (Stage cancelled: Job 176 cancelled )\n24/10/10 13:08:42 WARN BlockManager: Putting block rdd_932_1 failed due to exception org.apache.spark.TaskKilledException.\n24/10/10 13:08:42 WARN BlockManager: Block rdd_932_1 could not be removed as it was not found on disk or in memory\n24/10/10 13:08:42 WARN TaskSetManager: Lost task 1.0 in stage 909.0 (TID 3238) (8da198fe9dfc executor driver): TaskKilled (Stage cancelled: Job 176 cancelled )\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[109], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m final_prior_ord_train_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     result_usr_prod_df\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m      3\u001b[0m         result_df_with_time_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     )\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m final_prior_ord_train_df\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mfinal_prior_ord_train_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2837.count.\n: org.apache.spark.SparkException: Job 176 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"],"ename":"Py4JJavaError","evalue":"An error occurred while calling o2837.count.\n: org.apache.spark.SparkException: Job 176 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2731)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3013)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","output_type":"error"}]},{"cell_type":"code","source":"final_prior_ord_train_df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:07:04.093561Z","iopub.status.idle":"2024-10-10T13:07:04.097065Z","shell.execute_reply.started":"2024-10-10T13:07:04.096630Z","shell.execute_reply":"2024-10-10T13:07:04.096672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_prior_ord_train_df.write.csv('/kaggle/working/final_prior_ord_train_df.csv',header=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:07:04.310149Z","iopub.status.idle":"2024-10-10T13:07:04.310670Z","shell.execute_reply.started":"2024-10-10T13:07:04.310433Z","shell.execute_reply":"2024-10-10T13:07:04.310461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Show the file link\nFileLink(\"/kaggle/working/final_prior_ord_train_df.csv\")","metadata":{},"execution_count":null,"outputs":[]}]}