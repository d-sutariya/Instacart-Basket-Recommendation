{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":204944897,"sourceType":"kernelVersion"},{"sourceId":206537344,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub\n!pip install mlflow\n\n# !pip install pyngrok\n# !pip install shap\n","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:57:11.380745Z","iopub.execute_input":"2024-11-11T12:57:11.381205Z","iopub.status.idle":"2024-11-11T12:57:58.822439Z","shell.execute_reply.started":"2024-11-11T12:57:11.381161Z","shell.execute_reply":"2024-11-11T12:57:58.821218Z"}},"outputs":[{"name":"stdout","text":"Collecting dagshub\n  Downloading dagshub-0.3.42-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: PyYAML>=5 in /opt/conda/lib/python3.10/site-packages (from dagshub) (6.0.2)\nRequirement already satisfied: appdirs>=1.4.4 in /opt/conda/lib/python3.10/site-packages (from dagshub) (1.4.4)\nRequirement already satisfied: click>=8.0.4 in /opt/conda/lib/python3.10/site-packages (from dagshub) (8.1.7)\nRequirement already satisfied: httpx>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from dagshub) (0.27.0)\nRequirement already satisfied: GitPython>=3.1.29 in /opt/conda/lib/python3.10/site-packages (from dagshub) (3.1.43)\nRequirement already satisfied: rich>=13.1.0 in /opt/conda/lib/python3.10/site-packages (from dagshub) (13.7.1)\nCollecting dacite~=1.6.0 (from dagshub)\n  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tenacity>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from dagshub) (8.3.0)\nCollecting gql[requests] (from dagshub)\n  Downloading gql-3.5.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: dataclasses-json in /opt/conda/lib/python3.10/site-packages (from dagshub) (0.6.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from dagshub) (2.2.3)\nCollecting treelib>=1.6.4 (from dagshub)\n  Downloading treelib-1.7.0-py3-none-any.whl.metadata (1.3 kB)\nCollecting pathvalidate>=3.0.0 (from dagshub)\n  Downloading pathvalidate-3.2.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from dagshub) (2.9.0.post0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from dagshub) (1.26.100)\nCollecting dagshub-annotation-converter>=0.1.0 (from dagshub)\n  Downloading dagshub_annotation_converter-0.1.2-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from dagshub-annotation-converter>=0.1.0->dagshub) (5.3.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from dagshub-annotation-converter>=0.1.0->dagshub) (10.3.0)\nRequirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from dagshub-annotation-converter>=0.1.0->dagshub) (2.9.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from dagshub-annotation-converter>=0.1.0->dagshub) (4.12.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=3.1.29->dagshub) (4.0.11)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (4.4.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (3.7)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.23.0->dagshub) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.1.0->dagshub) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.1.0->dagshub) (2.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from treelib>=1.6.4->dagshub) (1.16.0)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3->dagshub)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->dagshub) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->dagshub) (0.6.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->dagshub) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json->dagshub) (0.9.0)\nCollecting graphql-core<3.3,>=3.2 (from gql[requests]->dagshub)\n  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: yarl<2.0,>=1.6 in /opt/conda/lib/python3.10/site-packages (from gql[requests]->dagshub) (1.9.4)\nCollecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: requests<3,>=2.26 in /opt/conda/lib/python3.10/site-packages (from gql[requests]->dagshub) (2.32.3)\nCollecting requests-toolbelt<2,>=1.0.0 (from gql[requests]->dagshub)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->dagshub) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->dagshub) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->dagshub) (2024.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.23.0->dagshub) (1.2.0)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->dagshub) (1.26.18)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (21.3)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.0->dagshub) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.0->dagshub) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.3.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\nRequirement already satisfied: multidict>=4.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.0.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (3.1.2)\nDownloading dagshub-0.3.42-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.5/251.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dacite-1.6.0-py3-none-any.whl (12 kB)\nDownloading dagshub_annotation_converter-0.1.2-py3-none-any.whl (33 kB)\nDownloading pathvalidate-3.2.1-py3-none-any.whl (23 kB)\nDownloading treelib-1.7.0-py3-none-any.whl (18 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gql-3.5.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: treelib, pathvalidate, graphql-core, dacite, backoff, requests-toolbelt, gql, botocore, dagshub-annotation-converter, dagshub\n  Attempting uninstall: dacite\n    Found existing installation: dacite 1.8.1\n    Uninstalling dacite-1.8.1:\n      Successfully uninstalled dacite-1.8.1\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.35.23\n    Uninstalling botocore-1.35.23:\n      Successfully uninstalled botocore-1.35.23\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.15.1 requires botocore<1.35.24,>=1.35.16, but you have botocore 1.29.165 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nydata-profiling 4.10.0 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed backoff-2.2.1 botocore-1.29.165 dacite-1.6.0 dagshub-0.3.42 dagshub-annotation-converter-0.1.2 gql-3.5.0 graphql-core-3.2.5 pathvalidate-3.2.1 requests-toolbelt-1.0.0 treelib-1.7.0\nCollecting mlflow\n  Downloading mlflow-2.17.2-py3-none-any.whl.metadata (29 kB)\nCollecting mlflow-skinny==2.17.2 (from mlflow)\n  Downloading mlflow_skinny-2.17.2-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.0.3)\nRequirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.13.3)\nRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (7.1.0)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.6)\nRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.2.3)\nRequirement already satisfied: pyarrow<18,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (17.0.0)\nRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow) (1.14.1)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow) (2.0.30)\nRequirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.10/site-packages (from mlflow) (3.1.4)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nCollecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.17.2->mlflow)\n  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (8.1.7)\nRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (3.0.0)\nCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.17.2->mlflow)\n  Downloading databricks_sdk-0.36.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (3.1.43)\nRequirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (7.0.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (1.25.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (1.25.0)\nRequirement already satisfied: packaging<25 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (21.3)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (3.20.3)\nRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow) (0.5.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.5)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.18)\nRequirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (3.0.4)\nRequirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow) (1.8.2)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow) (3.2.5)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.1.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (3.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.3)\nRequirement already satisfied: google-auth~=2.0 in /opt/conda/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow) (2.30.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow) (4.0.11)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.2->mlflow) (3.19.2)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow) (1.2.14)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow) (0.46b0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.17.2->mlflow) (2024.8.30)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow) (1.16.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow) (5.0.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow) (4.9)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow) (0.6.0)\nDownloading mlflow-2.17.2-py3-none-any.whl (26.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mlflow_skinny-2.17.2-py3-none-any.whl (5.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\nDownloading databricks_sdk-0.36.0-py3-none-any.whl (569 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nInstalling collected packages: graphql-relay, cachetools, gunicorn, graphene, databricks-sdk, mlflow-skinny, mlflow\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 4.2.4\n    Uninstalling cachetools-4.2.4:\n      Successfully uninstalled cachetools-4.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cachetools-5.3.3 databricks-sdk-0.36.0 graphene-3.4.3 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.17.2 mlflow-skinny-2.17.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import dagshub\nimport mlflow\nimport os\nimport time\nimport csv\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nimport h2o\nimport glob\nimport gc\nimport matplotlib.pyplot as plt\nfrom multiprocessing import Pool\nfrom mlflow.models.signature import infer_signature\nimport csv\nimport json\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom sklearn.metrics import roc_auc_score,precision_score,recall_score,f1_score,accuracy_score,log_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:57:58.824888Z","iopub.execute_input":"2024-11-11T12:57:58.825301Z","iopub.status.idle":"2024-11-11T12:58:04.001532Z","shell.execute_reply.started":"2024-11-11T12:57:58.825259Z","shell.execute_reply":"2024-11-11T12:58:04.000266Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# from model_trainer_script import ModelTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:04.002988Z","iopub.execute_input":"2024-11-11T12:58:04.003697Z","iopub.status.idle":"2024-11-11T12:58:04.009266Z","shell.execute_reply.started":"2024-11-11T12:58:04.003650Z","shell.execute_reply":"2024-11-11T12:58:04.007966Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"os.environ['MLFLOW_TRACKING_USERNAME'] = 'tnbmarketplace'\nos.environ['MLFLOW_TRACKING_PASSWORD'] = 'd7c1a451bfc46bf96fbbe5d723becf3955dfa2b3'\nos.environ['MLFLOW_TRACKING_URI'] = 'https://dagshub.com/tnbmarketplace/instacart_basket_analysis_exp_tracking.mlflow'\n!dagshub login --token \"d7c1a451bfc46bf96fbbe5d723becf3955dfa2b3\"","metadata":{"_uuid":"d263f1b5-dd41-4958-ad7f-4ea8f7dedb15","_cell_guid":"c45cd753-c853-4f3d-90bd-3fe67b223850","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:04.011285Z","iopub.execute_input":"2024-11-11T12:58:04.012577Z","iopub.status.idle":"2024-11-11T12:58:06.251720Z","shell.execute_reply.started":"2024-11-11T12:58:04.012512Z","shell.execute_reply":"2024-11-11T12:58:06.250208Z"}},"outputs":[{"name":"stdout","text":"\u001b[38;20m2024-11-11 12:58:05,989 - [INFO]  - [_client.py:1026] - HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"\u001b[0m\n✅ Token added successfully\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"dagshub.init(repo_name = \"instacart_basket_analysis_exp_tracking\",repo_owner = 'tnbmarketplace')","metadata":{"_uuid":"3c630320-f370-4792-9391-4a10c15478e9","_cell_guid":"486f1b7a-f896-495f-a0fc-e37a428d4f41","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:06.256341Z","iopub.execute_input":"2024-11-11T12:58:06.256867Z","iopub.status.idle":"2024-11-11T12:58:06.462877Z","shell.execute_reply.started":"2024-11-11T12:58:06.256812Z","shell.execute_reply":"2024-11-11T12:58:06.460652Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Accessing as tnbmarketplace\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as tnbmarketplace\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"tnbmarketplace/instacart_basket_analysis_exp_tracking\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"tnbmarketplace/instacart_basket_analysis_exp_tracking\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository tnbmarketplace/instacart_basket_analysis_exp_tracking initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository tnbmarketplace/instacart_basket_analysis_exp_tracking initialized!\n</pre>\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# !ngrok authtoken \"2kvaYw5ZiG5bL8iM8YJBVJPk1Ru_3C16mMgmpKEBYb28PPLUe\"\n# # Start Ngrok tunnel on H2O's port\n# ngrok_tunnel = ngrok.connect(8787)\n# print(\"H2O UI accessible at:\", ngrok_tunnel)","metadata":{"_uuid":"6ce13dd9-f459-444d-8064-ddb50dbf86db","_cell_guid":"eab0b1c4-6879-444e-b78a-5e4ee5d6a399","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:06.464428Z","iopub.execute_input":"2024-11-11T12:58:06.464784Z","iopub.status.idle":"2024-11-11T12:58:06.469383Z","shell.execute_reply.started":"2024-11-11T12:58:06.464746Z","shell.execute_reply":"2024-11-11T12:58:06.468207Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_time(start):\n    return time.time() - start","metadata":{"_uuid":"c0b2430b-167c-4cc2-8c02-084422aaf9b6","_cell_guid":"cc1d14df-9a63-4f45-b625-8c87c1baa5b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:06.471078Z","iopub.execute_input":"2024-11-11T12:58:06.471559Z","iopub.status.idle":"2024-11-11T12:58:06.482550Z","shell.execute_reply.started":"2024-11-11T12:58:06.471509Z","shell.execute_reply":"2024-11-11T12:58:06.481484Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dataset_version = \"1.2\" # dataset . split method\nmodel_version = \"1.1.1\" # algorithm . param version . used dataset \nread_as_xgb_dmatrix =  True\ntrain_xgb_gbm= True\ntrain_xgb_rf = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:06.484026Z","iopub.execute_input":"2024-11-11T12:58:06.484511Z","iopub.status.idle":"2024-11-11T12:58:06.496228Z","shell.execute_reply.started":"2024-11-11T12:58:06.484460Z","shell.execute_reply":"2024-11-11T12:58:06.495033Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\ntrain_features_name = []\nwith open(\"/kaggle/input/final-dataset-generator/train_set_columns.txt\",\"r\") as f:\n    csv_reader = csv.reader(f)\n    for row in csv_reader:\n        train_features_name = row\ntrain_features_name.remove('') \n\ntest_features_name = []\nwith open(\"/kaggle/input/final-dataset-generator/test_set_columns.txt\",\"r\") as f:\n    csv_reader = csv.reader(f)\n    for row in csv_reader:\n        test_features_name = row\ntest_features_name.remove('')  \n\ntrain_label_index = train_features_name.index('reordered')\n# test_label_index = test_features_name.index('reordered')\n\n\n\nfor i in range(len(test_features_name)):\n\n    if test_features_name[i] == 'time_mean_dow_count':\n       test_features_name[i] ='total_ord_count_p_dow'\n\n    if test_features_name[i] == 'time_mean_ohod_count':\n       test_features_name[i] ='total_ord_count_p_ohod'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:06.497787Z","iopub.execute_input":"2024-11-11T12:58:06.498294Z","iopub.status.idle":"2024-11-11T12:58:06.520387Z","shell.execute_reply.started":"2024-11-11T12:58:06.498222Z","shell.execute_reply":"2024-11-11T12:58:06.519274Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"xgb_booster = mlflow.xgboost.load_model(\"mlflow-artifacts:/c872cb0ab0ca48d481a71a9bcd2a2102/626567462ef746f8b45a4255ff51c5d1/artifacts/xgb_gbm_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:06.522072Z","iopub.execute_input":"2024-11-11T12:58:06.522575Z","iopub.status.idle":"2024-11-11T12:58:07.308703Z","shell.execute_reply.started":"2024-11-11T12:58:06.522521Z","shell.execute_reply":"2024-11-11T12:58:07.307256Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"460f2699ff2d4509a401826e187f9856"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"params = {\n    'max_depth': 1,\n    'min_child_weight': 389,\n    'verbose': -1,\n    'gamma': 438,\n    'eta': 0.0907183045377987,\n    'subsample': 0.10741019033611729,\n    'colsample_bytree': 0.336917979846298,\n    'colsample_bylevel': 0.118177830385349,\n    'lambda': 11,\n    'alpha': 89,\n    'booster': 'gbtree',\n    'tree_method': 'hist',\n    'objective': 'binary:logistic'\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:07.316839Z","iopub.execute_input":"2024-11-11T12:58:07.321297Z","iopub.status.idle":"2024-11-11T12:58:07.330970Z","shell.execute_reply.started":"2024-11-11T12:58:07.321228Z","shell.execute_reply":"2024-11-11T12:58:07.329363Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if read_as_xgb_dmatrix == True:\n    \n    train_features_name.remove('reordered')\n    # test_features_name.remove('reordered')\n\n    dtrain_2 = xgb.DMatrix(f\"/kaggle/input/final-dataset-generator/final_prior_train_set.csv/part-00000-37c713df-e906-4480-9322-5a9f58652182-c000.csv?format=csv&label_column={train_label_index}\",nthread=-1,feature_names=train_features_name)\n    # dtest_2 = xgb.DMatrix(f\"/kaggle/input/final-dataset-generator/featured_test_set.csv/part-00000-16b6977e-6b54-4aff-ad77-cb8ae65dfb2f-c000.csv?format=csv&label_column={test_label_index}\",nthread=-1,feature_names=test_features_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:58:07.333849Z","iopub.execute_input":"2024-11-11T12:58:07.335096Z","iopub.status.idle":"2024-11-11T12:59:43.236035Z","shell.execute_reply.started":"2024-11-11T12:58:07.335020Z","shell.execute_reply":"2024-11-11T12:59:43.235031Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# %% [code]\nimport time\nimport os\nimport gc\nimport json\nimport mlflow\nimport h2o\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, log_loss\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator\n\nclass ModelTrainer:\n    \n    def __init__(self, experiment_name, train_set, test_set=None, target_column='reordered'):\n        \"\"\"\n        Initializes the ModelTrainer with the experiment name, training set, and optional test set.\n        \n        Parameters:\n        experiment_name (str): The name of the MLflow experiment.\n        train_set (H2OFrame): The training dataset.\n        test_set (H2OFrame, optional): The test dataset. Defaults to None.\n        target_column (str): The name of the target column. Defaults to 'reordered'.\n        \"\"\"\n        self.train_set = train_set\n        self.test_set = test_set\n        self.exp_name = experiment_name\n        self.target_column = target_column  # Store the target column name\n        mlflow.set_experiment(self.exp_name)\n\n    def __log_details(self, y_true, preds, prev_commit_hash, params, model=None):\n        \"\"\"\n        Logs detailed information about the model's performance, environment, and dataset to MLflow.\n        \n        Parameters:\n        y_true (array-like): True values for the target variable.\n        preds (array-like): Predicted values.\n        prev_commit_hash (str): The commit hash for version control reference.\n        params (dict): Model hyperparameters.\n        model (object, optional): The trained model. Defaults to None.\n        \n        Logs:\n        - Precision, recall, F1 score, AUC, and log loss.\n        - Commit URL, environment, and dataset details.\n        \"\"\"\n        try:\n            # Log parameters\n            if params != None:\n                mlflow.log_params(params)\n            else:\n                mlflow.log_param(\"params\", None)\n\n            # Log metrics\n            pred_logits = [1 if pred >= 0.5 else 0 for pred in preds]\n            mlflow.log_metric(\"precision\", precision_score(y_true, pred_logits))\n            mlflow.log_metric(\"recall\", recall_score(y_true, pred_logits))\n            mlflow.log_metric(\"f1\", f1_score(y_true, pred_logits))\n            mlflow.log_metric(\"AUC\", roc_auc_score(y_true, preds))\n            mlflow.log_metric(\"logloss\", log_loss(y_true, preds))\n\n            # Log script URL with version \n            commit_url = \"https://github.com/d-sutariya/instacart_next_basket_prediction/tree/\" + prev_commit_hash\n            mlflow.log_param(\"repository url\", commit_url)\n\n            # Log environment \n            os.system(\"conda env export > conda.yaml\")\n            mlflow.log_artifact(\"conda.yaml\")\n\n            # Log dataset \n            dataset_path = \"https://www.kaggle.com/datasets/deepsutariya/instacart-exp-data\" \n            mlflow.log_param(\"dataset url\", dataset_path)\n\n            mlflow.end_run()\n\n        except Exception as e:\n            raise RuntimeError(f\"Error logging model details: {str(e)}\")\n    \n    def train_h2o_glm(self, prev_commit_hash, dataset_version, model_version, params=None):\n        \"\"\"\n        Trains a Generalized Linear Model (GLM) using H2O's binomial family.\n        \n        Parameters:\n        prev_commit_hash (str): The commit hash for version control.\n        dataset_version (str): The version of the dataset.\n        model_version (str): The version of the model.\n        params (dict, optional): Hyperparameters for the model. Defaults to None.\n        \n        Returns:\n        h2o_model: The trained H2O GLM model.\n        \"\"\"\n        try:\n            start = time.time()\n            \n            h2o_logistic_model = H2OGeneralizedLinearEstimator(family='binomial') \\\n                                .train(x=self.train_set.drop(\"reordered\").columns, y='reordered', training_frame=self.train_set)\n            duration = time.time() - start\n            \n            with mlflow.start_run():\n                mlflow.h2o.log_model(h2o_logistic_model, \"h2o_logistic_model\")\n                mlflow.log_param(\"family\", \"binomial\")\n                mlflow.log_param(\"alpha\", h2o_logistic_model.get_params()['alpha'])\n                mlflow.log_param(\"lambda\", h2o_logistic_model.get_params()['lambda'])\n                \n                mlflow.log_param(\"training_time\", duration)\n                mlflow.set_tag(\"dataset_version\", dataset_version)\n                mlflow.set_tag(\"model_version\", model_version)\n                mlflow.set_tag(\"algorithm\", \"h2o_glm\")\n                \n                progress = h2o_logistic_model.scoring_history().to_dict()\n                with open(\"loss_history.json\", \"w\") as f:\n                    json.dump(progress, f)\n                mlflow.log_artifact(\"loss_history.json\")\n                \n                if self.test_set != None:\n                    preds = h2o_logistic_model.predict(self.test_set).as_data_frame(use_multi_thread=True)['p1']\n                    y_true = self.test_set['reordered'].as_data_frame(use_multi_thread=True)\n                else:\n                    preds = h2o_logistic_model.predict(self.train_set).as_data_frame(use_multi_thread=True)['p1']\n                    y_true = self.train_set['reordered'].as_data_frame(use_multi_thread=True)\n\n                self.__log_details(y_true, preds, prev_commit_hash, params)\n                \n                del y_true, preds\n                gc.collect()\n\n            return h2o_logistic_model\n        \n        except Exception as e:\n            raise RuntimeError(f\"Error training H2O GLM: {str(e)}\")\n\n    def train_h2o_gbm(self, prev_commit_hash, dataset_version, model_version, params=None):\n        \"\"\"\n        Trains a Gradient Boosting Machine (GBM) model using H2O's GradientBoostingEstimator.\n        \n        Parameters:\n        prev_commit_hash (str): The commit hash for version control.\n        dataset_version (str): The version of the dataset.\n        model_version (str): The version of the model.\n        params (dict, optional): Hyperparameters for the model. Defaults to None.\n        \n        Returns:\n        h2o_model: The trained H2O GBM model.\n        \"\"\"\n        try:\n            if params != None and \"distribution\" not in params.keys():\n                params['distribution'] = 'bernoulli'\n\n            if params is None:\n                params = {'distribution': 'bernoulli'}\n            \n            start = time.time()\n            \n            if self.test_set != None:\n                h2o_gbm = H2OGradientBoostingEstimator(**params) \\\n                          .train(x=self.train_set.drop(\"reordered\").columns,\n                                 y='reordered',\n                                 training_frame=self.train_set,\n                                 validation_frame=self.test_set)\n            else:\n                h2o_gbm = H2OGradientBoostingEstimator(**params) \\\n                          .train(x=self.train_set.drop(\"reordered\").columns,\n                                 y='reordered',\n                                 training_frame=self.train_set)\n\n            duration = time.time() - start\n            \n            with mlflow.start_run():\n                mlflow.h2o.log_model(h2o_gbm, \"h2o_gbm_model\")\n                mlflow.log_params(params if params != None else {})\n                mlflow.log_param(\"training_time\", duration)\n                mlflow.set_tag(\"dataset_version\", dataset_version)\n                mlflow.set_tag(\"model_version\", model_version)\n                mlflow.set_tag(\"algorithm\", \"h2o_gbm\")\n                \n                progress = h2o_gbm.scoring_history().to_dict()\n                with open(\"loss_history.json\", \"w\") as f:\n                    json.dump(progress, f)\n                mlflow.log_artifact(\"loss_history.json\")\n                \n                if self.test_set != None:\n                    preds = h2o_gbm.predict(self.test_set).as_data_frame(use_multi_thread=True)['p1']\n                    y_true = self.test_set['reordered'].as_data_frame(use_multi_thread=True)\n                else:\n                    preds = h2o_gbm.predict(self.train_set).as_data_frame(use_multi_thread=True)['p1']\n                    y_true = self.train_set['reordered'].as_data_frame(use_multi_thread=True)\n\n                self.__log_details(y_true, preds, prev_commit_hash, params, h2o_gbm)\n\n            del y_true, preds\n            gc.collect()\n            return h2o_gbm\n        \n        except Exception as e:\n            raise RuntimeError(f\"Error training H2O GBM: {str(e)}\")\n\n    def train_xgb_gbm(self, prev_commit_hash, dataset_version, model_version, params=None):\n        \"\"\"\n        Trains a Gradient Boosting Machine (GBM) using XGBoost.\n        \n        Parameters:\n        prev_commit_hash (str): The commit hash for version control.\n        dataset_version (str): The version of the dataset.\n        model_version (str): The version of the model.\n        params (dict, optional): Hyperparameters for the model. Defaults to None.\n        \n        Returns:\n        xgb_model: The trained XGBoost GBM model.\n        \"\"\"\n        try:\n            if params != None:\n                if 'booster' not in params.keys():\n                    params['booster'] = 'gbtree'\n                if 'tree_method' not in params.keys():\n                    params['tree_method'] = 'hist'\n                if 'objective' not in params.keys():\n                    params['objective'] = 'binary:logistic'\n                if 'eval_metric' not in params.keys():\n                    params['eval_metric'] = 'logloss'\n\n            start = time.time()\n\n            if self.test_set != None:\n                watchlist = [(self.train_set, 'train'), (self.test_set, 'eval')]\n                xgb_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, evals=watchlist)\n            else:\n                xgb_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30,evals=[(self.train_set,'train')])\n\n            duration = time.time() - start\n\n            with mlflow.start_run():\n                \n                mlflow.xgboost.log_model(xgb_model, \"xgb_gbm_model\")\n                mlflow.log_params(params if params != None else {})\n                mlflow.log_param(\"training_time\", duration)\n                mlflow.set_tag(\"dataset_version\", dataset_version)\n                mlflow.set_tag(\"model_version\", model_version)\n                mlflow.set_tag(\"algorithm\", \"xgb_gbm\")\n\n                if self.test_set != None:\n                    preds = xgb_model.predict(self.test_set)\n                    y_true = self.test_set.get_label()\n                else:\n                    preds = xgb_model.predict(self.train_set)\n                    y_true = self.train_set.get_label()\n\n                self.__log_details(y_true, preds, prev_commit_hash, params, xgb_model)\n\n            del y_true, preds\n            gc.collect()\n            return xgb_model\n        \n        except Exception as e:\n            raise RuntimeError(f\"Error training XGBoost GBM: {str(e)}\")\n\n    def train_xgb_rf(self, prev_commit_hash, dataset_version, model_version, params=None):\n        \"\"\"\n        Trains a Random Forest using XGBoost's 'random forest' booster.\n        \n        Parameters:\n        prev_commit_hash (str): The commit hash for version control.\n        dataset_version (str): The version of the dataset.\n        model_version (str): The version of the model.\n        params (dict, optional): Hyperparameters for the model. Defaults to None.\n        \n        Returns:\n        xgb_model: The trained XGBoost Random Forest model.\n        \"\"\"\n        try:\n            if params != None:\n                if 'booster' not in params.keys():\n                    params['booster'] = 'gbtree'\n                if 'objective' not in params.keys():\n                    params['objective'] = 'binary:logistic'\n                if 'eval_metric' not in params.keys():\n                    params['eval_metric'] = 'logloss'\n\n            start = time.time()\n           \n            if self.test_set != None:\n                watchlist = [(self.train_set, 'train'), (self.test_set, 'eval')]\n                xgb_rf_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, evals=watchlist)\n            else:\n                xgb_rf_model = xgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30,evals=[(self.train_set,'train')])\n\n            duration = time.time() - start\n\n            with mlflow.start_run():\n                mlflow.xgboost.log_model(xgb_rf_model, \"xgb_rf_model\")\n                mlflow.log_params(params if params != None else {})\n                mlflow.log_param(\"training_time\", duration)\n                mlflow.set_tag(\"dataset_version\", dataset_version)\n                mlflow.set_tag(\"model_version\", model_version)\n                mlflow.set_tag(\"algorithm\", \"xgb_rf\")\n\n                if self.test_set != None:\n                    preds = xgb_rf_model.predict(self.test_set)\n                    y_true = self.test_set.get_label()\n                else:\n                    preds = xgb_rf_model.predict(self.train_set)\n                    y_true = self.test_set.get_label()\n\n                self.__log_details(y_true, preds, prev_commit_hash, params, xgb_rf_model)\n\n            del y_true, preds\n            gc.collect()\n            return xgb_rf_model\n        \n        except Exception as e:\n            raise RuntimeError(f\"Error training XGBoost RF: {str(e)}\")\n\n    def train_lgbm(self, prev_commit_hash, dataset_version, model_version, params=None):\n        \"\"\"\n        Trains a LightGBM model.\n        \n        Parameters:\n        prev_commit_hash (str): The commit hash for version control.\n        dataset_version (str): The version of the dataset.\n        model_version (str): The version of the model.\n        params (dict, optional): Hyperparameters for the model. Defaults to None.\n        \n        Returns:\n        lgb_model: The trained LightGBM model.\n        \"\"\"\n        try:\n            if params != None:\n                if 'objective' not in params.keys():\n                    params['objective'] = 'binary'\n\n            start = time.time()\n\n            \n            if self.test_set != None:\n                lgb_model = lgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, valid_sets=[self.test_set])\n            else:\n                lgb_model = lgb.train(params, self.train_set, num_boost_round=500, early_stopping_rounds=30, valid_sets=[self.train_set])\n\n            duration = time.time() - start\n\n            with mlflow.start_run():\n                \n                mlflow.lightgbm.log_model(lgb_model, \"lgb_model\")\n                mlflow.log_params(params if params != None else {})\n                mlflow.log_param(\"training_time\", duration)\n                mlflow.set_tag(\"dataset_version\", dataset_version)\n                mlflow.set_tag(\"model_version\", model_version)\n                mlflow.set_tag(\"algorithm\", \"lgbm\")\n\n            if self.test_set != None:\n                \n                preds = lgb_model.predict(self.test_set.get_data())\n                y_true = self.test_set.get_label()\n    \n                self.__log_details(y_true, preds, prev_commit_hash, params, lgb_gbm)\n            else:\n                \n                preds = lgb_model.predict(self.train_set.get_data())\n                y_true = self.train_set.get_label()\n    \n                self.__log_details(y_true, preds, prev_commit_hash, params, lgb_gbm)\n\n            del y_true, preds\n            gc.collect()\n            return lgb_model\n        \n        except Exception as e:\n            raise RuntimeError(f\"Error training LightGBM: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:59:43.241221Z","iopub.execute_input":"2024-11-11T12:59:43.243811Z","iopub.status.idle":"2024-11-11T12:59:43.477131Z","shell.execute_reply.started":"2024-11-11T12:59:43.243753Z","shell.execute_reply":"2024-11-11T12:59:43.475566Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if train_xgb_gbm == True:\n    model_trainer = ModelTrainer(\"final_instacart_training\",dtrain_2,params)\n    # paramss = {\n    #     'max_depth':1,\n    #     'min_child_weight':5,\n    #     'gamma':0.01,\n    #     'lambda':100,\n    #     'alpha':100\n    # }\n    xgb_gbm = model_trainer.train_xgb_gbm(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version,params)\n\nif train_xgb_rf == True:\n    model_trainer = ModelTrainer(\"instacart_training\",dtrain_2,dtest_2,\"reordered\")\n    xgb_rf = model_trainer.train_xgb_rf(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version)\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T12:59:43.482015Z","iopub.execute_input":"2024-11-11T12:59:43.482551Z","iopub.status.idle":"2024-11-11T12:59:44.430284Z","shell.execute_reply.started":"2024-11-11T12:59:43.482489Z","shell.execute_reply":"2024-11-11T12:59:44.428627Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 225\u001b[0m, in \u001b[0;36mModelTrainer.train_xgb_gbm\u001b[0;34m(self, prev_commit_hash, dataset_version, model_version, params)\u001b[0m\n\u001b[1;32m    224\u001b[0m     watchlist \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_set, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m--> 225\u001b[0m     xgb_model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwatchlist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/training.py:159\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    157\u001b[0m evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(evals) \u001b[38;5;28;01mif\u001b[39;00m evals \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m--> 159\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m start_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xgboost/core.py:1655\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, cache, model_file)\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(d, DMatrix):\n\u001b[0;32m-> 1655\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid cache item: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(d)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cache)\n\u001b[1;32m   1657\u001b[0m dmats \u001b[38;5;241m=\u001b[39m c_array(ctypes\u001b[38;5;241m.\u001b[39mc_void_p, [d\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m cache])\n","\u001b[0;31mTypeError\u001b[0m: ('invalid cache item: dict', [<xgboost.core.DMatrix object at 0x7ca942fe4400>, <xgboost.core.DMatrix object at 0x7ca942fe4400>, {'max_depth': 1, 'min_child_weight': 389, 'verbose': -1, 'gamma': 438, 'eta': 0.0907183045377987, 'subsample': 0.10741019033611729, 'colsample_bytree': 0.336917979846298, 'colsample_bylevel': 0.118177830385349, 'lambda': 11, 'alpha': 89, 'booster': 'gbtree', 'tree_method': 'hist', 'objective': 'binary:logistic', 'eval_metric': 'logloss'}])","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m     model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_instacart_training\u001b[39m\u001b[38;5;124m\"\u001b[39m,dtrain_2,params)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# paramss = {\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#     'max_depth':1,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#     'min_child_weight':5,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#     'alpha':100\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     xgb_gbm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_xgb_gbm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc391e8337f10ceb5870cb639159539f5e3497fbf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_xgb_rf \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     model_trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstacart_training\u001b[39m\u001b[38;5;124m\"\u001b[39m,dtrain_2,dtest_2,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreordered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[13], line 254\u001b[0m, in \u001b[0;36mModelTrainer.train_xgb_gbm\u001b[0;34m(self, prev_commit_hash, dataset_version, model_version, params)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xgb_model\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError training XGBoost GBM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: Error training XGBoost GBM: ('invalid cache item: dict', [<xgboost.core.DMatrix object at 0x7ca942fe4400>, <xgboost.core.DMatrix object at 0x7ca942fe4400>, {'max_depth': 1, 'min_child_weight': 389, 'verbose': -1, 'gamma': 438, 'eta': 0.0907183045377987, 'subsample': 0.10741019033611729, 'colsample_bytree': 0.336917979846298, 'colsample_bylevel': 0.118177830385349, 'lambda': 11, 'alpha': 89, 'booster': 'gbtree', 'tree_method': 'hist', 'objective': 'binary:logistic', 'eval_metric': 'logloss'}])"],"ename":"RuntimeError","evalue":"Error training XGBoost GBM: ('invalid cache item: dict', [<xgboost.core.DMatrix object at 0x7ca942fe4400>, <xgboost.core.DMatrix object at 0x7ca942fe4400>, {'max_depth': 1, 'min_child_weight': 389, 'verbose': -1, 'gamma': 438, 'eta': 0.0907183045377987, 'subsample': 0.10741019033611729, 'colsample_bytree': 0.336917979846298, 'colsample_bylevel': 0.118177830385349, 'lambda': 11, 'alpha': 89, 'booster': 'gbtree', 'tree_method': 'hist', 'objective': 'binary:logistic', 'eval_metric': 'logloss'}])","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"read_as_lgb_dataset = False\ntrain_lgb = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if read_as_lgb_dataset == True:\n\n    # train_features_name.remove('reordered')\n#     test_features_name.remove('reordered')\n    \n    dtrain_2 = lgb.Dataset(\"/kaggle/input/final-dataset-generator/final_prior_train_set.csv/part-00000-95d78719-42e1-441c-a4e1-d662676226a3-c000.csv\",params={'label_column':train_label_index},free_raw_data=False).construct()\n    dtest_2 = pd.read_csv(\"/kaggle/input/final-dataset-generator/featured_test_set.csv/part-00000-1b691469-02d1-4ed1-8b50-ec568a4bf8a9-c000.csv\",header = None)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if train_lgb == True:\n    model_trainer = ModelTrainer(\"final_instacart_training\",[dtrain_2])\n    lgb_gbm = model_trainer.train_lgb_gbm(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"read_as_h2o_frame = False\ntrain_h2o = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if read_as_h2o_frame == True:\n    h2o.init(max_mem_size = '25G')\n    dtrain_2 = h2o.import_file(\"/kaggle/input/instacart-dataset-transformer-cv2/final_prior_train_set.csv/part-00000-444aa52f-9c41-4a07-8342-7f82d9fcf6ac-c000.csv\")\n    dtest_2 = h2o.import_file(\"/kaggle/input/instacart-dataset-transformer-cv2/featured_test_set.csv/part-00000-017a6365-a37c-4ce4-8bf4-96e8daa33570-c000.csv\")\n    dtrain_2.col_names = train_features_name\n    dtest_2.col_names = test_features_name\n    dtrain_2['reordered'] = dtrain_2['reordered'].asfactor()\n    dtest_2['reordered'] = dtest_2['reordered'].asfactor()","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if train_h2o == True:\n    \n    model_trainer = ModelTrainer(\"instacart_training\",[dtrain_2],dtest_2,\"reordered\")\n    h2o_gbm = model_trainer.train_h2o_glm(\"c391e8337f10ceb5870cb639159539f5e3497fbf\",dataset_version,model_version)","metadata":{"scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}